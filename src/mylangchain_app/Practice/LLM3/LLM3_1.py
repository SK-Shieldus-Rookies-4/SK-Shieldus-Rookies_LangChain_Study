import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_upstage import ChatUpstage, UpstageEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# ğŸŒŸ Mac OS í™˜ê²½ì—ì„œ FAISS/OpenMP ì¶©ëŒ ë°©ì§€
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'


def main():
    # 0. API í‚¤ ë¡œë“œ
    load_dotenv()
    UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")

    if not UPSTAGE_API_KEY:
        print("âŒ ì˜¤ë¥˜: UPSTAGE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    # 0ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ
    FILE_PATH = "../data/ì½˜í…ì¸ ë¶„ìŸí•´ê²°_ì‚¬ë¡€.pdf"
    print("âœ… 0ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ ì‹œì‘...")
    loader = PyPDFLoader(FILE_PATH)
    documents = loader.load()
    print(f"   -> {len(documents)} í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")

    # 1ë‹¨ê³„: ë¬¸ì„œ ë¶„í•  ì„¤ì •
    print("\nâœ… 1ë‹¨ê³„: ë¬¸ì„œ ë¶„í•  ì‹œì‘...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,       # ê¶Œì¥ ë²”ìœ„: 1200~1800ì
        chunk_overlap=300,     # 200~400ì ê¶Œì¥
        separators=[
            "\nã€ì‚¬ê±´ê°œìš”ã€‘",
            "\nã€ìŸì ì‚¬í•­ã€‘",
            "\nã€ì²˜ë¦¬ê²½ìœ„ã€‘",
            "\nã€ì²˜ë¦¬ê²°ê³¼ã€‘",
            "\nâ– ", "\n\n", "\n", ".", " ", ""
        ]
    )
    split_docs = text_splitter.split_documents(documents)
    print(f"   -> ë¶„í• ëœ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(split_docs)}")

    # 2ë‹¨ê³„: ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    print("\nâœ… 2ë‹¨ê³„: ì„ë² ë”© ëª¨ë¸ ìƒì„±...")
    embeddings = UpstageEmbeddings(
        model="solar-embedding-1-large",
        upstage_api_key=UPSTAGE_API_KEY
    )
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    print("   -> ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")

    # 3ë‹¨ê³„: ê²€ìƒ‰ê¸° ì„¤ì •
    print("\nâœ… 3ë‹¨ê³„: ê²€ìƒ‰ê¸° ì„¤ì •...")
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 5}
    )
    print("   -> ê²€ìƒ‰ê¸° ì„¤ì • ì™„ë£Œ (k=5)")

    # 4ë‹¨ê³„: LLM ì„¤ì •
    print("\nâœ… 4ë‹¨ê³„: LLM ì„¤ì •...")
    llm = ChatUpstage(
        model="solar-pro",
        base_url="https://api.upstage.ai/v1",
        temperature=0.2,
        upstage_api_key=UPSTAGE_API_KEY
    )
    print("   -> LLM ì„¤ì • ì™„ë£Œ")

    # 5ë‹¨ê³„: ë²•ë¥  ìë¬¸ í”„ë¡¬í”„íŠ¸ ì‘ì„±
    print("\nâœ… 5ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ì‘ì„±...")
    prompt_template = """
ë‹¹ì‹ ì€ ì½˜í…ì¸  ë¶„ì•¼ ì „ë¬¸ ë²•ë¥  ìë¬¸ê°€ì…ë‹ˆë‹¤. 
ì•„ë˜ ë¶„ìŸì¡°ì • ì‚¬ë¡€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë²•ë¥  ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ê´€ë ¨ ë¶„ìŸì‚¬ë¡€:
{context}

ìƒë‹´ ë‚´ìš©: {query}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ì œì‹œëœ ì‚¬ë¡€ë“¤ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ê´€ë ¨ ë²•ë ¹ì´ë‚˜ ì¡°í•­ì´ ìˆë‹¤ë©´ ëª…ì‹œí•˜ì„¸ìš”
3. ë¹„ìŠ·í•œ ì‚¬ë¡€ì˜ ì²˜ë¦¬ê²½ìœ„ì™€ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”
4. ì‹¤ë¬´ì  í•´ê²°ë°©ì•ˆì„ ë‹¨ê³„ë³„ë¡œ ì œì‹œí•˜ì„¸ìš”
5. ì‚¬ë¡€ì— ì—†ëŠ” ë‚´ìš©ì€ "ì œì‹œëœ ì‚¬ë¡€ì§‘ì—ì„œëŠ” í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”

ì „ë¬¸ ë²•ë¥  ìë¬¸:"""

    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "query"]
    )
    print("   -> í”„ë¡¬í”„íŠ¸ ì‘ì„± ì™„ë£Œ")

    # 6ë‹¨ê³„: QA ì²´ì¸ ìƒì„±
    print("\nâœ… 6ë‹¨ê³„: QA ì²´ì¸ ìƒì„±...")
    

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        return_source_documents=True,
        input_key="query",   # ì™¸ë¶€ì—ì„œ ë„£ëŠ” í‚¤
        chain_type_kwargs={
            "question_key": "query"   # ë‚´ë¶€ combine_documents_chainì—ì„œ ì“¸ í‚¤
        }
    )
    print("   -> QA ì²´ì¸ ìƒì„± ì™„ë£Œ")
    print("QA Chain input keys:", qa_chain.input_keys)

    # 7ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_questions = [
        "ì˜¨ë¼ì¸ ê²Œì„ì—ì„œ ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì•„ì´í…œì´ ì‚¬ë¼ì¡ŒëŠ”ë°, ê²Œì„íšŒì‚¬ê°€ ë³µêµ¬ë¥¼ ê±°ë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆë‚˜ìš”?",
        "ì¸í„°ë„· ê°•ì˜ë¥¼ ì¤‘ë„ í•´ì§€í•˜ë ¤ê³  í•˜ëŠ”ë° ê³¼ë„í•œ ìœ„ì•½ê¸ˆì„ ìš”êµ¬ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì •ë‹¹í•œê°€ìš”?",
        "ë¬´ë£Œì²´í—˜ í›„ ìë™ìœ¼ë¡œ ìœ ë£Œì „í™˜ë˜ì–´ ìš”ê¸ˆì´ ì²­êµ¬ë˜ì—ˆìŠµë‹ˆë‹¤. í™˜ë¶ˆ ê°€ëŠ¥í•œê°€ìš”?",
        "ë¯¸ì„±ë…„ìê°€ ë¶€ëª¨ ë™ì˜ ì—†ì´ ê²Œì„ ì•„ì´í…œì„ êµ¬ë§¤í–ˆìŠµë‹ˆë‹¤. í™˜ë¶ˆë°›ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆë‚˜ìš”?",
        "ì˜¨ë¼ì¸ êµìœ¡ ì„œë¹„ìŠ¤ê°€ ê´‘ê³ ì™€ ë‹¤ë¥´ê²Œ ì œê³µë˜ì–´ ê³„ì•½ì„ í•´ì§€í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ê°€ëŠ¥í•œê°€ìš”?"
    ]

    print("\nâœ… 7ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì‹¤í–‰...")
    for q in test_questions:
        print("\n===================================================")
        print(f"ğŸ” ì§ˆë¬¸: {q}")
        print("===================================================")
        result = qa_chain.invoke({"query": q})
        print("âœ… [ì „ë¬¸ ë²•ë¥  ì¡°ì–¸]")
        print(result["result"])

        print("\nğŸ“œ [ì°¸ì¡° ë¬¸ì„œ]")
        for i, doc in enumerate(result["source_documents"], 1):
            print(f"  {i}. í˜ì´ì§€: {doc.metadata.get('page', 'Unknown')}")


    # 8ë‹¨ê³„: ì„ íƒ - ë¶„ìŸ ìœ í˜• ë¶„ë¥˜ í•¨ìˆ˜
    def classify_dispute_type(query):
        game_keywords = ["ê²Œì„", "ì•„ì´í…œ", "ê³„ì •", "ìºë¦­í„°", "ë ˆë²¨", "ê¸¸ë“œ", "ì˜¨ë¼ì¸ê²Œì„"]
        elearning_keywords = ["ê°•ì˜", "ì˜¨ë¼ì¸êµìœ¡", "ì´ëŸ¬ë‹", "ìˆ˜ê°•", "í™˜ë¶ˆ", "í™”ìƒêµìœ¡"]
        web_keywords = ["ì›¹ì‚¬ì´íŠ¸", "ë¬´ë£Œì²´í—˜", "ìë™ê²°ì œ", "êµ¬ë…", "ì‚¬ì´íŠ¸"]

        query_lower = query.lower()

        if any(keyword in query_lower for keyword in game_keywords):
            return "ê²Œì„"
        elif any(keyword in query_lower for keyword in elearning_keywords):
            return "ì´ëŸ¬ë‹"
        elif any(keyword in query_lower for keyword in web_keywords):
            return "ì›¹ì½˜í…ì¸ "
        else:
            return "ê¸°íƒ€"

    # ë¶„ìŸ ìœ í˜• í…ŒìŠ¤íŠ¸
    print("\nâœ… 8ë‹¨ê³„: ë¶„ìŸ ìœ í˜• ë¶„ë¥˜ í…ŒìŠ¤íŠ¸...")
    for q in test_questions:
        print(f"'{q}' â†’ {classify_dispute_type(q)}")


if __name__ == "__main__":
    main()