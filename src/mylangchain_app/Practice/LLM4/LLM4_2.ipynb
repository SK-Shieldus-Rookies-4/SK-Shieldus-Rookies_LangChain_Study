{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "29fe1c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 10ê°œì˜ ë©”ë‰´ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "--- ê²°ê³¼ ì¶œë ¥ ---\n",
      "[USER] ì´ ì¹´í˜ì˜ ë©”ë‰´ ê°€ê²©ì„ ì•Œê³  ì‹¶ì–´ìš”.\n",
      "[ASSISTANT] ë¬¸ì˜í•˜ì‹  ê´€ë ¨ ë©”ë‰´ë“¤ì˜ ê°€ê²© ì •ë³´ì…ë‹ˆë‹¤:\n",
      "ë°”ë‹ë¼ ë¼ë–¼: â‚©6,000\n",
      "ë…¹ì°¨ ë¼ë–¼: â‚©5,800\n",
      "ì•„ì´ìŠ¤ ì•„ë©”ë¦¬ì¹´ë…¸: â‚©4,500\n",
      "ì¹´í˜ë¼ë–¼: â‚©5,500\n",
      "ì¹´ë¼ë©œ ë§ˆí‚¤ì•„í† : â‚©6,500\n",
      "í”„ë¼í‘¸ì¹˜ë…¸: â‚©7,000\n",
      "ì¹´í‘¸ì¹˜ë…¸: â‚©5,000\n",
      "í‹°ë¼ë¯¸ìˆ˜: â‚©7,500\n",
      "ì•„ë©”ë¦¬ì¹´ë…¸: â‚©4,500\n",
      "ì½œë“œë¸Œë£¨: â‚©5,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/4gd1xpgn60n2x9960_jc16y00000gn/T/ipykernel_37299/803721339.py:122: DeprecationWarning: 'count' is passed as positional argument\n",
      "  content = re.sub(r'^\\d+\\.\\s*.+\\n', '', block, 1).strip()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Annotated, Literal, List, Optional\n",
    "from typing_extensions import TypedDict\n",
    "import os \n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Document í´ë°± (langchain.schema.Documentê°€ ì—†ì„ ë•Œ ëŒ€ë¹„)\n",
    "try:\n",
    "    from langchain.schema import Document  # type: ignore\n",
    "except Exception:\n",
    "    from dataclasses import dataclass\n",
    "    @dataclass\n",
    "    class Document:\n",
    "        page_content: str\n",
    "        metadata: dict\n",
    "\n",
    "# === ê°„ë‹¨í•œ ë²¡í„°ìŠ¤í† ì–´ ëŒ€ì²´ (í‚¤ì›Œë“œ ê¸°ë°˜ 'ì˜ë¯¸ë¡ ì ' ê²€ìƒ‰) ===\n",
    "class SimpleKeywordVectorStore:\n",
    "    \"\"\"í† í° ê²¹ì¹¨(ìì¹´ë“œ ìœ ì‚¬ë„)ìœ¼ë¡œ í‰ë‚´ë‚´ëŠ” ê²½ëŸ‰ ê²€ìƒ‰ê¸°.\"\"\"\n",
    "    def __init__(self):\n",
    "        self._docs: List[Document] = []\n",
    "        self._tokens: List[set] = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _tokenize(text: str) -> set:\n",
    "        # í•œê¸€/ìˆ«ì/í†µí™”ê¸°í˜¸ ì •ë„ë§Œ ë‚¨ê¸°ê³  í† í°í™”\n",
    "        text = re.sub(r\"[^ê°€-í£a-zA-Z0-9â‚©,.\\s]\", \" \", text)\n",
    "        toks = set(t.strip() for t in re.split(r\"\\s+\", text) if t.strip())\n",
    "        return toks\n",
    "\n",
    "    def add_documents(self, docs: List[Document]) -> None:\n",
    "        for d in docs:\n",
    "            self._docs.append(d)\n",
    "            self._tokens.append(self._tokenize(d.page_content + \" \" + \" \".join(f\"{k}:{v}\" for k, v in d.metadata.items())))\n",
    "\n",
    "    def similarity_search(self, query: str, k: int = 4) -> List[Document]:\n",
    "        if not self._docs:\n",
    "            return []\n",
    "        q = self._tokenize(query)\n",
    "        scores = []\n",
    "        for i, toks in enumerate(self._tokens):\n",
    "            inter = len(q & toks)\n",
    "            union = len(q | toks) if q or toks else 1\n",
    "            score = inter / union \n",
    "            scores.append((score, i))\n",
    "        scores.sort(reverse=True, key=lambda x: x[0])\n",
    "        # top = [self._docs[i] for s, i in scores[:k] if s > 0]\n",
    "        # ğŸš¨ğŸš¨ğŸš¨ (s > 0 í•„í„° ì œê±°) ğŸš¨ğŸš¨ğŸš¨\n",
    "        top = [self._docs[i] for s, i in scores[:k]]            \n",
    "        return top\n",
    "\n",
    "# === ë©”ë‰´ ì •ë³´ ì¶”ì¶œ ===\n",
    "def extract_menu_info(doc: Document) -> dict:\n",
    "    \"\"\"Vector DB ë¬¸ì„œì—ì„œ êµ¬ì¡°í™”ëœ ë©”ë‰´ ì •ë³´ ì¶”ì¶œ\"\"\"\n",
    "    content = doc.page_content\n",
    "    menu_name = doc.metadata.get('menu_name', 'Unknown')\n",
    "\n",
    "    # 'ê°€ê²©: â‚©[ìˆ«ì]' íŒ¨í„´ì„ ì°¾ìŒ\n",
    "    price_match = re.search(r'ê°€ê²©:\\s*(â‚©[\\d,]+)', content)\n",
    "    # 'ì„¤ëª…: [ë‚´ìš©]' íŒ¨í„´ì„ ì°¾ìŒ (ì¤„ë°”ê¿ˆ í¬í•¨)\n",
    "    description_match = re.search(r'ì„¤ëª…:\\s*(.+?)(?:\\n\\d|\\Z)', content, re.DOTALL) \n",
    "\n",
    "    return {\n",
    "        \"name\": menu_name,\n",
    "        \"price\": price_match.group(1).strip() if price_match else \"ê°€ê²© ì •ë³´ ì—†ìŒ\",\n",
    "        \"description\": description_match.group(1).strip() if description_match else \"ì„¤ëª… ì—†ìŒ\"\n",
    "    }\n",
    "\n",
    "# === ë¬¸ì˜ ìœ í˜• ë¶„ë¥˜ ===\n",
    "def classify_user_query(user_message: str) -> Literal[\"ê°€ê²© ë¬¸ì˜\", \"ì¶”ì²œ ìš”ì²­\", \"ë©”ë‰´ ë¬¸ì˜\", \"ê¸°íƒ€\"]:\n",
    "    msg = user_message.strip()\n",
    "    if \"ê°€ê²©\" in msg or \"ì–¼ë§ˆ\" in msg or \"ë¹„ì‹¸\" in msg or \"ê°€ê²©í‘œ\" in msg:\n",
    "        return \"ê°€ê²© ë¬¸ì˜\"\n",
    "    if \"ì¶”ì²œ\" in msg or \"ë­ê°€ ì¢‹ì•„\" in msg or \"ì¸ê¸°\" in msg or \"ë² ìŠ¤íŠ¸\" in msg:\n",
    "        return \"ì¶”ì²œ ìš”ì²­\"\n",
    "    if \"ë©”ë‰´\" in msg or \"ìˆë‚˜ìš”\" in msg or \"íŒŒë‚˜ìš”\" in msg or \"ì„¤ëª…\" in msg:\n",
    "        return \"ë©”ë‰´ ë¬¸ì˜\"\n",
    "    return \"ê¸°íƒ€\"\n",
    "\n",
    "# === ìƒíƒœ ì •ì˜ ===\n",
    "class CafeState(TypedDict, total=False):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    route: Literal[\"ê°€ê²© ë¬¸ì˜\", \"ì¶”ì²œ ìš”ì²­\", \"ë©”ë‰´ ë¬¸ì˜\", \"ê¸°íƒ€\"]\n",
    "    last_query: str\n",
    "    search_results: List[dict] \n",
    "\n",
    "# ğŸš¨ğŸš¨ğŸš¨ íŒŒì¼ ì½ê¸° í•¨ìˆ˜ ì¶”ê°€ ğŸš¨ğŸš¨ğŸš¨\n",
    "def read_menu_data_from_file(file_path: str) -> Optional[str]:\n",
    "    \"\"\"ì§€ì •ëœ íŒŒì¼ ê²½ë¡œì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return None\n",
    "\n",
    "# ğŸš¨ğŸš¨ğŸš¨ ë°ì´í„° íŒŒì‹± í•¨ìˆ˜ ğŸš¨ğŸš¨ğŸš¨\n",
    "def load_menu_data(data_string: str) -> List[Document]:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\"\"\"\n",
    "    docs = []\n",
    "    # ë©”ë‰´ ë²ˆí˜¸ì™€ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: '1. ì•„ë©”ë¦¬ì¹´ë…¸')\n",
    "    menu_blocks = re.split(r'\\n(?=\\d+\\.\\s)', data_string.strip())\n",
    "    \n",
    "    for block in menu_blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "            \n",
    "        name_match = re.match(r'^\\d+\\.\\s*(.+)', block)\n",
    "        if not name_match:\n",
    "            continue\n",
    "            \n",
    "        menu_name = name_match.group(1).strip()\n",
    "        \n",
    "        # ë©”ë‰´ ì´ë¦„ì´ í¬í•¨ëœ ì²« ì¤„ì„ ì œê±°í•˜ê³  ë‚˜ë¨¸ì§€ ë‚´ìš©ì„ Document contentë¡œ ì‚¬ìš©\n",
    "        content = re.sub(r'^\\d+\\.\\s*.+\\n', '', block, 1).strip()\n",
    "        content = content.replace('\\t', '').replace('â€¢ ', '').replace('Â ', '')\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=content, \n",
    "                metadata={\"menu_name\": menu_name}\n",
    "            )\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "# ğŸš¨ğŸš¨ğŸš¨ ë°ì´í„° ì†ŒìŠ¤ (ë²¡í„°ìŠ¤í† ì–´) ì¤€ë¹„ ğŸš¨ğŸš¨ğŸš¨\n",
    "MENU_FILE_PATH = '/Applications/SK-Shieldus-Rookies_Cloud_Study/mylangchain/mylangchain-app/src/mylangchain_app/Practice/data/cafe_menu_data.txt' # â­â­ íŒŒì¼ ê²½ë¡œ ì„¤ì • â­â­\n",
    "\n",
    "# 1. íŒŒì¼ì—ì„œ ë°ì´í„° ì½ê¸°\n",
    "menu_data_text = read_menu_data_from_file(MENU_FILE_PATH)\n",
    "\n",
    "MENU_DB = SimpleKeywordVectorStore()\n",
    "\n",
    "if menu_data_text:\n",
    "    # 2. ì½ì€ ë°ì´í„°ë¥¼ Documentë¡œ íŒŒì‹±\n",
    "    menu_documents = load_menu_data(menu_data_text)\n",
    "    # 3. ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€\n",
    "    MENU_DB.add_documents(menu_documents)\n",
    "    print(f\"âœ… {len(menu_documents)}ê°œì˜ ë©”ë‰´ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. ì±—ë´‡ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# === ë…¸ë“œ êµ¬í˜„  ===\n",
    "def node_classify(state: CafeState) -> CafeState:\n",
    "    \"\"\"ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë¡œ ë¬¸ì˜ ìœ í˜•ì„ ê²°ì •í•˜ê³  routeì— ê¸°ë¡.\"\"\"\n",
    "    last_user = None\n",
    "    for m in reversed(state[\"messages\"]):\n",
    "        if isinstance(m, HumanMessage):\n",
    "            last_user = m\n",
    "            break\n",
    "    user_text = (last_user.content if last_user else \"\").strip()\n",
    "    route = classify_user_query(user_text) if user_text else \"ê¸°íƒ€\"\n",
    "    state[\"route\"] = route\n",
    "    state[\"last_query\"] = user_text\n",
    "    return state\n",
    "\n",
    "def node_search(state: CafeState) -> CafeState:\n",
    "    \"\"\"ë¬¸ì˜ ìœ í˜•ë³„ ê²€ìƒ‰ ì „ëµ ìˆ˜í–‰ í›„, ì¤‘ê°„ ê²°ê³¼(êµ¬ì¡°í™”) ë©”ì‹œì§€ë¡œ ì €ì¥.\"\"\"\n",
    "    route = state.get(\"route\", \"ê¸°íƒ€\")\n",
    "    user_message = state.get(\"last_query\", \"\")\n",
    "\n",
    "    all_menu_names = \" \".join([d.metadata['menu_name'] for d in MENU_DB._docs]) \n",
    "\n",
    "    if route == \"ê°€ê²© ë¬¸ì˜\":\n",
    "        docs = MENU_DB.similarity_search(f\"{user_message} ë©”ë‰´ ê°€ê²© {all_menu_names}\", k=10) \n",
    "        \n",
    "    elif route == \"ì¶”ì²œ ìš”ì²­\":\n",
    "        enhanced_query = user_message + \" \" + all_menu_names\n",
    "        docs = MENU_DB.similarity_search(enhanced_query, k=3)\n",
    "        if not docs:\n",
    "            docs = MENU_DB.similarity_search(f\"ì¸ê¸° ë©”ë‰´ {all_menu_names}\", k=3)\n",
    "            \n",
    "    elif route == \"ë©”ë‰´ ë¬¸ì˜\":\n",
    "        docs = MENU_DB.similarity_search(user_message, k=4)\n",
    "    else:\n",
    "        docs = MENU_DB.similarity_search(user_message, k=3)\n",
    "\n",
    "    if not docs:\n",
    "        state[\"messages\"] += [AIMessage(content=\"ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")]\n",
    "        state[\"search_results\"] = [] \n",
    "        return state\n",
    "\n",
    "    # infos = [extract_menu_info(d) for d in docs[:5]]\n",
    "     # ğŸš¨ğŸš¨ğŸš¨ docs[:5]ë¥¼ docs[:] ë˜ëŠ” docsë¡œ ë³€ê²½ ğŸš¨ğŸš¨ğŸš¨\n",
    "    infos = [extract_menu_info(d) for d in docs] # docs ì „ì²´ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •\n",
    "    state[\"search_results\"] = infos  \n",
    "    return state\n",
    "\n",
    "\n",
    "def node_respond(state: CafeState) -> CafeState:\n",
    "    route = state.get(\"route\", \"ê¸°íƒ€\")\n",
    "    infos: List[dict] = state.get(\"search_results\", [])\n",
    "    query = state.get(\"last_query\", \"\")\n",
    "\n",
    "    if not infos:\n",
    "        state[\"messages\"] += [AIMessage(content=\"ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.\")]\n",
    "        return state\n",
    "\n",
    "    # â­â­ ì§ˆë¬¸ì—ì„œ íŠ¹ì • ë©”ë‰´ ì´ë¦„ ì°¾ê¸° - ì „ì²´ ë©”ë‰´ DBë¥¼ ëŒ€ìƒìœ¼ë¡œ í™•ì¸ â­â­\n",
    "    target_menu_name = None\n",
    "    # ì „ì²´ ë©”ë‰´ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (ê²€ìƒ‰ ê²°ê³¼ì— ê´€ê³„ ì—†ì´)\n",
    "    all_known_menus = sorted([d.metadata['menu_name'] for d in MENU_DB._docs], key=len, reverse=True) \n",
    "    \n",
    "    # ì§ˆë¬¸ì— ì „ì²´ ë©”ë‰´ ì¤‘ í•˜ë‚˜ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "    for menu_name in all_known_menus:\n",
    "        if menu_name in query: \n",
    "            target_menu_name = menu_name\n",
    "            break\n",
    "\n",
    "    is_specific_query = (target_menu_name is not None)\n",
    "    top = infos[0] # ê¸°ë³¸ê°’ì€ ê²€ìƒ‰ ê²°ê³¼ 1ìœ„\n",
    "\n",
    "    if is_specific_query:\n",
    "        # íŠ¹ì • ë©”ë‰´ê°€ ì–¸ê¸‰ë˜ì—ˆë‹¤ë©´, í•´ë‹¹ ë©”ë‰´ ì •ë³´ë¥¼ ì „ì²´ DBì—ì„œ ì°¾ì•„ì„œ topìœ¼ë¡œ ì„¤ì •\n",
    "        target_doc = next((d for d in MENU_DB._docs if d.metadata.get('menu_name') == target_menu_name), None)\n",
    "        if target_doc:\n",
    "            top = extract_menu_info(target_doc)\n",
    "        # ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ë©”ë‰´ê°€ ë°œê²¬ë˜ì§€ ì•Šì•„ë„ topì€ infos[0]ìœ¼ë¡œ ìœ ì§€ë¨ (ì˜ˆì™¸ ì²˜ë¦¬)\n",
    "    \n",
    "    # â­â­ 'ê°€ê²© ë¬¸ì˜' ì‹œ íŠ¹ì • ë©”ë‰´ ì–¸ê¸‰ ì—¬ë¶€ì— ë”°ë¼ ì‘ë‹µ ë¶„ê¸° â­â­\n",
    "    if route == \"ê°€ê²© ë¬¸ì˜\":\n",
    "        if is_specific_query:\n",
    "            # â¡ï¸ íŠ¹ì • ë©”ë‰´ ê°€ê²© ìš”ì²­: í•´ë‹¹ ë©”ë‰´ë§Œ ì‘ë‹µ (í‹°ë¼ë¯¸ìˆ˜, ë°”ë‹ë¼ ë¼ë–¼ ë“±)\n",
    "            msg = f\"{top['name']}ì˜ ê°€ê²©ì€ {top['price']}ì…ë‹ˆë‹¤.\"\n",
    "        else:\n",
    "            # â¡ï¸ ì¼ë°˜ ê°€ê²© ëª©ë¡ ìš”ì²­: ê²€ìƒ‰ëœ ìƒìœ„ ë©”ë‰´ë“¤ì„ ì‘ë‹µ (\"ë©”ë‰´ ê°€ê²© ì•Œë ¤ì¤˜\" ë“±)\n",
    "            price_list = [f\"{info['name']}: {info['price']}\" for info in infos[:10]]\n",
    "            \n",
    "            if price_list:\n",
    "                 msg = \"ë¬¸ì˜í•˜ì‹  ê´€ë ¨ ë©”ë‰´ë“¤ì˜ ê°€ê²© ì •ë³´ì…ë‹ˆë‹¤:\\n\" + \"\\n\".join(price_list)\n",
    "            else:\n",
    "                 msg = \"ë©”ë‰´ ê°€ê²© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "            \n",
    "    elif route == \"ì¶”ì²œ ìš”ì²­\":\n",
    "        msg = f\"ì¶”ì²œ ë©”ë‰´: {top['name']} â€” {top['description']} ({top['price']}).\"\n",
    "        \n",
    "    elif route == \"ë©”ë‰´ ë¬¸ì˜\":\n",
    "        msg = f\"{top['name']} ì„¤ëª…: {top['description']} (ê°€ê²©: {top['price']}).\"\n",
    "\n",
    "    else: # route == \"ê¸°íƒ€\"\n",
    "        msg = f\"ìš”ì²­ì— ë§ëŠ” ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê´€ë ¨ ë©”ë‰´: {top['name']} â€” {top['description']} ({top['price']}).\"\n",
    "\n",
    "    state[\"messages\"] += [AIMessage(content=msg)]\n",
    "    return state\n",
    "\n",
    "# === ê·¸ë˜í”„ êµ¬ì„± ===\n",
    "graph = StateGraph(CafeState)\n",
    "graph.add_node(\"classify\", node_classify)\n",
    "graph.add_node(\"search\", node_search)\n",
    "graph.add_node(\"respond\", node_respond)\n",
    "\n",
    "graph.add_edge(START, \"classify\")\n",
    "graph.add_edge(\"classify\", \"search\")\n",
    "graph.add_edge(\"search\", \"respond\")\n",
    "graph.add_edge(\"respond\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "print(\"\\n--- ê²°ê³¼ ì¶œë ¥ ---\")\n",
    "state1: CafeState = {\n",
    "     \"messages\": [HumanMessage(content=\"ì´ ì¹´í˜ì˜ ë©”ë‰´ ê°€ê²©ì„ ì•Œê³  ì‹¶ì–´ìš”.\")]\n",
    "    #  \"messages\": [HumanMessage(content=\"ì´ ì¹´í˜ì—ì„œ ì œì¼ ì¸ê¸° ë§ì€ ë©”ë‰´ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”\")]\n",
    "    #  \"messages\": [HumanMessage(content=\"ì¹´ë¼ë©œ ë§ˆí‚¤ì•„í† ì— ë­ê°€ ë“¤ì–´ê°€ëŠ”ì§€ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\")]\n",
    "    # \"messages\": [HumanMessage(content=\"í‹°ë¼ë¯¸ìˆ˜ëŠ” ê°€ê²©ì´ ì–¼ë§ˆì¸ê°€ìš”?\")]\n",
    "}\n",
    "result1 = app.invoke(state1)\n",
    "for m in result1[\"messages\"]:\n",
    "    role = \"USER\" if isinstance(m, HumanMessage) else \"ASSISTANT\"\n",
    "    print(f\"[{role}] {m.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-75wTJiYg-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
