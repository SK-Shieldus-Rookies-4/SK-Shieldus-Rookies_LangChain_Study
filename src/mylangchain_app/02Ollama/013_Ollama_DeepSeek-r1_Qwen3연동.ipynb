{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry langchain langchain-ollama langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로컬 Ollama로 설치한 deepseek-r1 모델과 ExaOne3 또는 Qwen3 모델을 사용하기\n",
    "##### ollama run deepseek-r1:7b\n",
    "##### ollama run exaone3.5\n",
    "##### ollama run qwen3:1.7b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 최신버전 LangChain에서는 ChatOllama와 RunnableSequence(prompt | llm) 를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### deepseeek 모델 9.9 와 9.11 크기 비교문제  (영어로 질문, invoke()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to determine which number is larger between 9.9 and 9.11.\n",
      "\n",
      "To make an accurate comparison, it's helpful to express both numbers with the same number of decimal places. \n",
      "\n",
      "I'll convert 9.9 to 9.90 by adding a zero to the right. Now, both numbers have two decimal places: 9.90 and 9.11.\n",
      "\n",
      "Next, I'll compare each corresponding digit from left to right. The whole number part is the same for both numbers (9), so I move on to comparing the tenths place. \n",
      "\n",
      "In 9.90, the tenths digit is 9, while in 9.11 it's 1. Since 9 is greater than 1, 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Express Both Numbers with the Same Number of Decimal Places\n",
      "- **9.9** can be written as **9.90** to have two decimal places.\n",
      "- **9.11** already has two decimal places.\n",
      "\n",
      "So, we're comparing:\n",
      "- **9.90**\n",
      "- **9.11**\n",
      "\n",
      "### Step 2: Compare the Numbers Digit by Digit\n",
      "1. **Compare the whole number part:**\n",
      "   - Both numbers have **9** as their whole number part.\n",
      "   \n",
      "2. **Compare the tenths place:**\n",
      "   - **9.90** has a **9** in the tenths place.\n",
      "   - **9.11** has a **1** in the tenths place.\n",
      "   \n",
      "   Since **9 > 1**, **9.90** is larger than **9.11**.\n",
      "\n",
      "### Conclusion\n",
      "\\[\n",
      "\\boxed{9.9}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "try:\n",
    "    deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)    # 모델 호출\n",
    "    response = deepseek.invoke(\"which is bigger between 9.9 and 9.11?\")\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### qwen3:1.7b 모델 9.9 와 9.11 크기 비교문제  (한글로 질문, invoke()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think about how to approach this. \n",
      "\n",
      "First, I know that when comparing decimals, it's usually helpful to look at the digits from left to right. Both numbers are decimals, so they have the same number of digits after the decimal point, right? Wait, 9.9 has one decimal place, and 9.11 has two. Hmm, so maybe I need to consider the number of decimal places?\n",
      "\n",
      "But wait, actually, both numbers have one decimal place in the first case and two in the second. So when comparing them, even though they have different numbers of decimal places, the value of the number with more decimal places might be larger. But I need to check.\n",
      "\n",
      "Let me write them down:\n",
      "\n",
      "9.9 is the same as 9.90. So if I convert 9.9 to have two decimal places, it becomes 9.90. Then 9.11 is already two decimal places. So comparing 9.90 and 9.11. \n",
      "\n",
      "Starting from the left, the first digit is 9 in both. Then the second digit: 9 vs 1. Wait, no, actually, 9.90 is 9.90, and 9.11 is 9.11. So the first decimal place for 9.90 is 9, and for 9.11 it's 1. Wait, no, the second decimal place for 9.90 is 0, right? Because 9.90 is 9.90, so the first decimal place is 9, the second is 0. \n",
      "\n",
      "So comparing 9.90 and 9.11. The first digit after the decimal is 9 vs 1. Since 9 is greater than 1, then 9.90 is greater than 9.11. Therefore, 9.9 is bigger than 9.11.\n",
      "\n",
      "Wait, but maybe I should check if there's any trick here. For example, sometimes people might confuse the decimal places. Let me confirm. 9.9 is 9.90, and 9.11 is 9.11. So yes, 9.90 is larger than 9.11 because the first decimal place is 9 vs 1. So the answer is 9.9.\n",
      "\n",
      "Alternatively, if I think in terms of whole numbers: 9.9 is 9 + 0.9, and 9.11 is 9 + 0.11. So 0.9 is larger than 0.11, so 9.9 is bigger.\n",
      "\n",
      "Another way: subtract them. 9.9 minus 9.11 equals 0.79. So since the result is positive, 9.9 is larger.\n",
      "\n",
      "I think that's solid. So the answer is 9.9.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다. \n",
      "\n",
      "**이유:**  \n",
      "- 9.9는 9.90으로 변환할 수 있으며, 9.11보다 0.90이 더 큽니다.  \n",
      "- 9.11은 9.11로, 9.90보다 0.79 더 작은 값입니다.  \n",
      "- 따라서 **9.9 > 9.11**입니다.\n",
      "\n",
      "**답변:**  \n",
      "**9.9**\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "try:\n",
    "    #exaone = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5, n_gpu_layers=0, batch_size=128)\n",
    "    exaone = ChatOllama(model=\"qwen3:1.7b\", temperature=0.5)\n",
    "    # 모델 호출\n",
    "    response = exaone.invoke(\"9.9와 9.11 중 무엇이 더 큰가요?\")\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### deepseeek 모델 9.9 와 9.11 크기 비교문제  (영어로 질문, stream()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "To determine which number is larger between 9.9 and 9.11, I'll start by aligning their decimal places for a fair comparison.\n",
      "\n",
      "First, I can express both numbers with three decimal places:\n",
      "- 9.9 becomes 9.900.\n",
      "- 9.11 remains as it is since it already has two decimal places.\n",
      "\n",
      "Next, I'll compare the whole number parts of both numbers. Both have the same whole number part, which is 9.\n",
      "\n",
      "Since the whole numbers are equal, I'll move on to comparing the tenths place:\n",
      "- In 9.900, the tenths digit is 9.\n",
      "- In 9.11, the tenths digit is 1.\n",
      "\n",
      "Comparing these digits, 9 is greater than 1, which means 9.900 (or 9.9) is larger than 9.11.\n",
      "\n",
      "Therefore, between 9.9 and 9.11, 9.9 is the bigger number.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, follow these steps:\n",
      "\n",
      "1. **Align Decimal Places:**\n",
      "   - Convert both numbers to have the same number of decimal places for accurate comparison.\n",
      "     - **9.9** can be written as **9.900**.\n",
      "     - **9.11** remains **9.11**.\n",
      "\n",
      "2. **Compare Whole Numbers:**\n",
      "   - Both numbers have the same whole number part (**9**).\n",
      "\n",
      "3. **Compare Tenths Place:**\n",
      "   - Look at the tenths digit:\n",
      "     - In **9.900**, the tenths digit is **9**.\n",
      "     - In **9.11**, the tenths digit is **1**.\n",
      "\n",
      "4. **Determine Larger Number:**\n",
      "   - Since **9 > 1**, **9.900** (or **9.9**) is larger than **9.11**.\n",
      "\n",
      "**Final Answer:**  \n",
      "\\boxed{9.9}"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "To determine which number is larger between 9.9 and 9.11, I'll start by aligning their decimal places for a fair comparison.\n",
       "\n",
       "First, I can express both numbers with three decimal places:\n",
       "- 9.9 becomes 9.900.\n",
       "- 9.11 remains as it is since it already has two decimal places.\n",
       "\n",
       "Next, I'll compare the whole number parts of both numbers. Both have the same whole number part, which is 9.\n",
       "\n",
       "Since the whole numbers are equal, I'll move on to comparing the tenths place:\n",
       "- In 9.900, the tenths digit is 9.\n",
       "- In 9.11, the tenths digit is 1.\n",
       "\n",
       "Comparing these digits, 9 is greater than 1, which means 9.900 (or 9.9) is larger than 9.11.\n",
       "\n",
       "Therefore, between 9.9 and 9.11, 9.9 is the bigger number.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.9** and **9.11**, follow these steps:\n",
       "\n",
       "1. **Align Decimal Places:**\n",
       "   - Convert both numbers to have the same number of decimal places for accurate comparison.\n",
       "     - **9.9** can be written as **9.900**.\n",
       "     - **9.11** remains **9.11**.\n",
       "\n",
       "2. **Compare Whole Numbers:**\n",
       "   - Both numbers have the same whole number part (**9**).\n",
       "\n",
       "3. **Compare Tenths Place:**\n",
       "   - Look at the tenths digit:\n",
       "     - In **9.900**, the tenths digit is **9**.\n",
       "     - In **9.11**, the tenths digit is **1**.\n",
       "\n",
       "4. **Determine Larger Number:**\n",
       "   - Since **9 > 1**, **9.900** (or **9.9**) is larger than **9.11**.\n",
       "\n",
       "**Final Answer:**  \n",
       "\\boxed{9.9}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### deepseeek 모델 9.9 와 9.11 크기 비교문제  (한글로 질문, stream()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the decimal numbers 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align them by adding an extra zero to 9.9, making it 9.90.\n",
      "\n",
      "Now, both numbers are 9.90 and 9.11.\n",
      "\n",
      "Starting from the left, both have the same digit (9).\n",
      "\n",
      "Next, compare the tenths place: 9 in 9.90 versus 1 in 9.11.\n",
      "\n",
      "Since 9 is greater than 1, 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "</think>\n",
      "\n",
      " comparison between \\( 9.9 \\) and \\( 9.11 \\):\n",
      "\n",
      "1. **Align the Decimal Places:**\n",
      "   - To compare these decimals accurately, it's helpful to align their decimal places by adding an extra zero to the shorter number.\n",
      "\n",
      "2. **Express Both Numbers with Two Decimal Places:**\n",
      "   - \\( 9.9 \\) becomes \\( 9.90 \\)\n",
      "   - \\( 9.11 \\) remains \\( 9.11 \\)\n",
      "\n",
      "3. **Compare Digit by Digit:**\n",
      "   - Compare the whole number parts first:\n",
      "     - Both numbers have \\( 9 \\) in the units place.\n",
      "   - Move to the tenths place:\n",
      "     - Tenths place of \\( 9.90 \\): \\( 9 \\)\n",
      "     - Tenths place of \\( 9.11 \\): \\( 1 \\)\n",
      "\n",
      "4. **Determine Which is Larger:**\n",
      "   - Since \\( 9 \\) (from \\( 9.90 \\)) is greater than \\( 1 \\) (from \\( 9.11 \\)), the number \\( 9.90 \\) is larger.\n",
      "\n",
      "5. **Conclusion:**\n",
      "   - Therefore, \\( 9.9 \\) is greater than \\( 9.11 \\).\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9 > 9.11}\n",
      "\\]"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "First, I need to compare the decimal numbers 9.9 and 9.11.\n",
       "\n",
       "Both numbers have the same whole number part, which is 9.\n",
       "\n",
       "To make a fair comparison, I'll align them by adding an extra zero to 9.9, making it 9.90.\n",
       "\n",
       "Now, both numbers are 9.90 and 9.11.\n",
       "\n",
       "Starting from the left, both have the same digit (9).\n",
       "\n",
       "Next, compare the tenths place: 9 in 9.90 versus 1 in 9.11.\n",
       "\n",
       "Since 9 is greater than 1, 9.90 is larger than 9.11.\n",
       "\n",
       "Therefore, 9.9 is greater than 9.11.\n",
       "</think>\n",
       "\n",
       " comparison between \\( 9.9 \\) and \\( 9.11 \\):\n",
       "\n",
       "1. **Align the Decimal Places:**\n",
       "   - To compare these decimals accurately, it's helpful to align their decimal places by adding an extra zero to the shorter number.\n",
       "\n",
       "2. **Express Both Numbers with Two Decimal Places:**\n",
       "   - \\( 9.9 \\) becomes \\( 9.90 \\)\n",
       "   - \\( 9.11 \\) remains \\( 9.11 \\)\n",
       "\n",
       "3. **Compare Digit by Digit:**\n",
       "   - Compare the whole number parts first:\n",
       "     - Both numbers have \\( 9 \\) in the units place.\n",
       "   - Move to the tenths place:\n",
       "     - Tenths place of \\( 9.90 \\): \\( 9 \\)\n",
       "     - Tenths place of \\( 9.11 \\): \\( 1 \\)\n",
       "\n",
       "4. **Determine Which is Larger:**\n",
       "   - Since \\( 9 \\) (from \\( 9.90 \\)) is greater than \\( 1 \\) (from \\( 9.11 \\)), the number \\( 9.90 \\) is larger.\n",
       "\n",
       "5. **Conclusion:**\n",
       "   - Therefore, \\( 9.9 \\) is greater than \\( 9.11 \\).\n",
       "\n",
       "\\[\n",
       "\\boxed{9.9 > 9.11}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### qwen3:1.7b 모델 9.9 와 9.11 크기 비교문제  (한글로 질문, stream()함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking whether 9.9 is bigger than 9.11. Hmm, I need to compare these two numbers. \n",
      "\n",
      "First, I should look at the whole numbers. Both are 9, so that's the same. Then, the decimal parts. The first number, 9.9, has a decimal part of 0.9, and the second one, 9.11, has 0.11. \n",
      "\n",
      "Wait, 0.9 is equal to 0.90, right? So if I compare 0.90 and 0.11, obviously 0.90 is bigger. So even though the whole numbers are the same, the decimal parts make 9.9 larger. \n",
      "\n",
      "But let me double-check. Maybe I should convert them to the same decimal places. 9.11 is 9.110, and 9.9 is 9.900. Comparing the tenths place: 9 vs 1. Wait, no, the tenths place for 9.9 is 9, and for 9.11 it's 1. Wait, that's not right. Wait, 9.9 is 9.90, so the tenths place is 9, and 9.11 is 9.11. So in the tenths place, 9 vs 1. So 9 is bigger than 1. So 9.9 is bigger than 9.11. \n",
      "\n",
      "Wait, but maybe I'm confusing the decimal places. Let me think again. 9.9 is 9.90, and 9.11 is 9.11. So when comparing 9.90 and 9.11, the first decimal place is 9 vs 1, so 9.90 is larger. Yes, that's correct. \n",
      "\n",
      "So the answer should be 9.9 is bigger than 9.11. I think that's right. But maybe I should write it in a clear way. Let me make sure I didn't make any mistakes. \n",
      "\n",
      "Alternatively, if I think about it in terms of fractions. 9.9 is 9 + 9/10, and 9.11 is 9 + 11/100. So 9/10 is 0.9 and 11/100 is 0.11. So 0.9 is definitely bigger than 0.11. Therefore, 9.9 is bigger. \n",
      "\n",
      "Yes, that makes sense. So the conclusion is 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9는 9.11보다 더 큰 수입니다.  \n",
      "- 두 수의 정수 부분은 모두 9입니다.  \n",
      "- 소수 부분을 비교하면, 9.9는 0.9, 9.11은 0.11입니다.  \n",
      "- 0.9는 0.11보다 크므로, 9.9 > 9.11입니다.  \n",
      "\n",
      "**답변:** 9.9가 더 큰 수입니다."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, let's see. The user is asking whether 9.9 is bigger than 9.11. Hmm, I need to compare these two numbers. \n",
       "\n",
       "First, I should look at the whole numbers. Both are 9, so that's the same. Then, the decimal parts. The first number, 9.9, has a decimal part of 0.9, and the second one, 9.11, has 0.11. \n",
       "\n",
       "Wait, 0.9 is equal to 0.90, right? So if I compare 0.90 and 0.11, obviously 0.90 is bigger. So even though the whole numbers are the same, the decimal parts make 9.9 larger. \n",
       "\n",
       "But let me double-check. Maybe I should convert them to the same decimal places. 9.11 is 9.110, and 9.9 is 9.900. Comparing the tenths place: 9 vs 1. Wait, no, the tenths place for 9.9 is 9, and for 9.11 it's 1. Wait, that's not right. Wait, 9.9 is 9.90, so the tenths place is 9, and 9.11 is 9.11. So in the tenths place, 9 vs 1. So 9 is bigger than 1. So 9.9 is bigger than 9.11. \n",
       "\n",
       "Wait, but maybe I'm confusing the decimal places. Let me think again. 9.9 is 9.90, and 9.11 is 9.11. So when comparing 9.90 and 9.11, the first decimal place is 9 vs 1, so 9.90 is larger. Yes, that's correct. \n",
       "\n",
       "So the answer should be 9.9 is bigger than 9.11. I think that's right. But maybe I should write it in a clear way. Let me make sure I didn't make any mistakes. \n",
       "\n",
       "Alternatively, if I think about it in terms of fractions. 9.9 is 9 + 9/10, and 9.11 is 9 + 11/100. So 9/10 is 0.9 and 11/100 is 0.11. So 0.9 is definitely bigger than 0.11. Therefore, 9.9 is bigger. \n",
       "\n",
       "Yes, that makes sense. So the conclusion is 9.9 is larger than 9.11.\n",
       "</think>\n",
       "\n",
       "9.9는 9.11보다 더 큰 수입니다.  \n",
       "- 두 수의 정수 부분은 모두 9입니다.  \n",
       "- 소수 부분을 비교하면, 9.9는 0.9, 9.11은 0.11입니다.  \n",
       "- 0.9는 0.11보다 크므로, 9.9 > 9.11입니다.  \n",
       "\n",
       "**답변:** 9.9가 더 큰 수입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepSeek의 추론 능력과 Qwen3의 한글 생성 능력 결합하기\n",
    "* DeepSeek는 태그 안에서 이루어지는 추론을 기반으로 다른 LLM 대비 높은 성능을 발휘합니다.\n",
    "* 하지만 Ollama에서 제공하는 deepseek r1 모델은 한국어 생성 능력이 부족합니다.\n",
    "* DeepSeek의 추론 능력과 Qwen3의 한글 생성 능력 결합해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='qwen3:1.7b' temperature=0.7\n"
     ]
    }
   ],
   "source": [
    "#generation_model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.7)\n",
    "generation_model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.7)\n",
    "print(generation_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangGraph 로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\\n\\n        당신의 작업은 다음과 같습니다:\\n        - 질문에 제공된 추론을 신중하게 분석하세요.\\n        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\\n        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\\n        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\\n\\n        지침:\\n        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\\n        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\\n        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\\n        - 도움이 되고 전문적인 톤을 유지하세요.\\n\\n        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        질문: {question}\\n        추론: {thinking}\\n        '), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#LangGraph에서 State 사용자정의 클래스는 노드 간의 정보를 전달하는 틀입니다. \n",
    "#노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야 할 정보를 미리 정의힙니다. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문에 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langgraph.graph.state.StateGraph'>\n",
      "<class 'langgraph.graph.state.CompiledStateGraph'>\n"
     ]
    }
   ],
   "source": [
    "#DeepSeek를 통해서 추론 부분까지만 생성합니다. Node\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwen를 통해서 결과 출력 부분을 생성합니다. Node\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "print(type(graph_builder))\n",
    "print(type(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers start with 9, so the whole numbers are equal. But the decimal parts matter here. \n",
      "\n",
      "I remember that when comparing decimals, you go digit by digit starting from the left. So, 9.9 is the same as 9.90 when you add a zero. Then, comparing the tenths place: both have 9. But the hundredths place is different. The first number has 9 in the hundredths, and the second has 1 in the hundredths. Wait, no, wait. Wait, 9.9 is 9.90, right? So the second number is 9.11. So the hundredths place for 9.90 is 0, and for 9.11 it's 1. Oh, right! So 0 is less than 1, so 9.90 is less than 9.11? Wait, no, wait. Wait, 9.9 is 9.90, and 9.11 is 9.11. So comparing 9.90 and 9.11. The tenths place: 9 vs 1. Since 9 is greater than 1, 9.90 is greater than 9.11. So the answer should be 9.9 is bigger. \n",
      "\n",
      "Wait, but the user's initial thought was that 9.9 is larger. But in the initial thinking process, the assistant thought that 9.9 is larger because the hundredths place of 9.90 (which is 0) is less than 1 in 9.11. So the conclusion is 9.9 is larger. So the answer is 9.9. \n",
      "\n",
      "But the assistant's initial response said that 9.9 is larger because the hundredths place of 9.90 (which is 0) is less than 1 in 9.11. So yes, that's correct. So the final answer is 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 9.9이 더 큰 수입니다. 두 수는 모두 9를 기준으로 시작하므로 정수 부분은 같지만, 소수 부분을 비교해야 합니다. 9.9는 9.90와 같으며, 9.11은 9.11입니다. 소수 부분에서 9.90의 9는 9.11의 1보다 크므로, 9.90(9.9)이 9.11(9.11)보다 더 큰 수입니다. 따라서 9.9가 더 큰 수입니다.\n",
      "{'question': '9.9와 9.11 중 무엇이 더 큰가요?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nBoth numbers have the same whole number part, which is 9.\\n\\nTo make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\\n\\nNow, comparing each digit after the decimal point:\\n\\n- The tenths place: Both numbers have 9.\\n- The hundredths place: 9 has 9 and 11 has 0.\\n\\nSince 9 is greater than 0 in the hundredths place for 9.90 compared to 9.11, it follows that 9.9 is larger than 9.11.\\n\", 'answer': \"<think>\\nOkay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers start with 9, so the whole numbers are equal. But the decimal parts matter here. \\n\\nI remember that when comparing decimals, you go digit by digit starting from the left. So, 9.9 is the same as 9.90 when you add a zero. Then, comparing the tenths place: both have 9. But the hundredths place is different. The first number has 9 in the hundredths, and the second has 1 in the hundredths. Wait, no, wait. Wait, 9.9 is 9.90, right? So the second number is 9.11. So the hundredths place for 9.90 is 0, and for 9.11 it's 1. Oh, right! So 0 is less than 1, so 9.90 is less than 9.11? Wait, no, wait. Wait, 9.9 is 9.90, and 9.11 is 9.11. So comparing 9.90 and 9.11. The tenths place: 9 vs 1. Since 9 is greater than 1, 9.90 is greater than 9.11. So the answer should be 9.9 is bigger. \\n\\nWait, but the user's initial thought was that 9.9 is larger. But in the initial thinking process, the assistant thought that 9.9 is larger because the hundredths place of 9.90 (which is 0) is less than 1 in 9.11. So the conclusion is 9.9 is larger. So the answer is 9.9. \\n\\nBut the assistant's initial response said that 9.9 is larger because the hundredths place of 9.90 (which is 0) is less than 1 in 9.11. So yes, that's correct. So the final answer is 9.9 is larger than 9.11.\\n</think>\\n\\n9.9와 9.11 중 9.9이 더 큰 수입니다. 두 수는 모두 9를 기준으로 시작하므로 정수 부분은 같지만, 소수 부분을 비교해야 합니다. 9.9는 9.90와 같으며, 9.11은 9.11입니다. 소수 부분에서 9.90의 9는 9.11의 1보다 크므로, 9.90(9.9)이 9.11(9.11)보다 더 큰 수입니다. 따라서 9.9가 더 큰 수입니다.\"}\n",
      "==> 생성된 답변: \n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, both numbers start with 9, so the whole numbers are equal. But the decimal parts matter here. \n",
      "\n",
      "I remember that when comparing decimals, you go digit by digit starting from the left. So, 9.9 is the same as 9.90 when you add a zero. Then, comparing the tenths place: both have 9. But the hundredths place is different. The first number has 9 in the hundredths, and the second has 1 in the hundredths. Wait, no, wait. Wait, 9.9 is 9.90, right? So the second number is 9.11. So the hundredths place for 9.90 is 0, and for 9.11 it's 1. Oh, right! So 0 is less than 1, so 9.90 is less than 9.11? Wait, no, wait. Wait, 9.9 is 9.90, and 9.11 is 9.11. So comparing 9.90 and 9.11. The tenths place: 9 vs 1. Since 9 is greater than 1, 9.90 is greater than 9.11. So the answer should be 9.9 is bigger. \n",
      "\n",
      "Wait, but the user's initial thought was that 9.9 is larger. But in the initial thinking process, the assistant thought that 9.9 is larger because the hundredths place of 9.90 (which is 0) is less than 1 in 9.11. So the conclusion is 9.9 is larger. So the answer is 9.9. \n",
      "\n",
      "But the assistant's initial response said that 9.9 is larger because the hundredths place of 9.90 (which is 0) is less than 1 in 9.11. So yes, that's correct. So the final answer is 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 9.9이 더 큰 수입니다. 두 수는 모두 9를 기준으로 시작하므로 정수 부분은 같지만, 소수 부분을 비교해야 합니다. 9.9는 9.90와 같으며, 9.11은 9.11입니다. 소수 부분에서 9.90의 9는 9.11의 1보다 크므로, 9.90(9.9)이 9.11(9.11)보다 더 큰 수입니다. 따라서 9.9가 더 큰 수입니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 입력 데이터\n",
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "# invoke()를 사용하여 그래프 호출\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"==> 생성된 답변: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Install Pyppeteer to use the Pyppeteer method: `pip install pyppeteer`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnest_asyncio\u001b[39;00m \n\u001b[32m      5\u001b[39m nest_asyncio.apply()\n\u001b[32m      7\u001b[39m display(\n\u001b[32m      8\u001b[39m     Image(\n\u001b[32m      9\u001b[39m         \u001b[38;5;66;03m#graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMermaidDrawMethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPYPPETEER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     )\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langchain_core/runnables/graph.py:702\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m    693\u001b[39m     draw_mermaid_png,\n\u001b[32m    694\u001b[39m )\n\u001b[32m    696\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    697\u001b[39m     curve_style=curve_style,\n\u001b[32m    698\u001b[39m     node_colors=node_colors,\n\u001b[32m    699\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    700\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    701\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langchain_core/runnables/graph_mermaid.py:304\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Draws a Mermaid graph as PNG using provided syntax.\u001b[39;00m\n\u001b[32m    282\u001b[39m \n\u001b[32m    283\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    301\u001b[39m \u001b[33;03m    ValueError: If an invalid draw method is provided.\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m draw_method == MermaidDrawMethod.PYPPETEER:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     img_bytes = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_render_mermaid_using_pyppeteer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m    310\u001b[39m     img_bytes = _render_mermaid_using_api(\n\u001b[32m    311\u001b[39m         mermaid_syntax,\n\u001b[32m    312\u001b[39m         output_file_path=output_file_path,\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m         retry_delay=retry_delay,\n\u001b[32m    316\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/asyncio/futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.13/asyncio/tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langchain_core/runnables/graph_mermaid.py:338\u001b[39m, in \u001b[36m_render_mermaid_using_pyppeteer\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, padding, device_scale_factor)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAS_PYPPETEER:\n\u001b[32m    337\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInstall Pyppeteer to use the Pyppeteer method: `pip install pyppeteer`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    340\u001b[39m browser = \u001b[38;5;28;01mawait\u001b[39;00m launch()\n\u001b[32m    341\u001b[39m page = \u001b[38;5;28;01mawait\u001b[39;00m browser.newPage()\n",
      "\u001b[31mImportError\u001b[39m: Install Pyppeteer to use the Pyppeteer method: `pip install pyppeteer`."
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "import nest_asyncio \n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        #graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.PYPPETEER)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LangGraph 실행 시작\n",
      "==================================================\n",
      "[DEBUG] 입력 질문: 9.9와 9.11 중 무엇이 더 큰가요?\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 466\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as...\n",
      "[DEBUG] generate 함수 - 질문: 9.9와 9.11 중 무엇이 더 큰가요?\n",
      "[DEBUG] generate 함수 - 추론 길이: 466\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "오류 발생: model \"qwen2.5:1.5b\" not found, try pulling it first (status code: 404)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/ss/4gd1xpgn60n2x9960_jc16y00000gn/T/ipykernel_33633/2706828140.py\", line 133, in main\n",
      "    result = graph.invoke(inputs)\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langgraph/pregel/main.py\", line 3026, in invoke\n",
      "    for chunk in self.stream(\n",
      "                 ~~~~~~~~~~~^\n",
      "        input,\n",
      "        ^^^^^^\n",
      "    ...<10 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    ):\n",
      "    ^\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langgraph/pregel/main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "             ~~~~~~~~~~~^\n",
      "        [t for t in loop.tasks.values() if not t.writes],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        schedule_task=loop.accept_push,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ):\n",
      "    ^\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langgraph/pregel/_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "    ~~~~~~~~~~~~~~^\n",
      "        t,\n",
      "        ^^\n",
      "    ...<10 lines>...\n",
      "        },\n",
      "        ^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langgraph/_internal/_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langgraph/_internal/_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"/var/folders/ss/4gd1xpgn60n2x9960_jc16y00000gn/T/ipykernel_33633/2706828140.py\", line 109, in generate\n",
      "    response = generation_model.invoke(messages)\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "    ~~~~~~~~~~~~~~~~~~~~^\n",
      "        [self._convert_input(input)],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    ).generations[0][0],\n",
      "    ^\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        m,\n",
      "        ^^\n",
      "    ...<2 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
      "    )\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langchain_ollama/chat_models.py\", line 824, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "        messages, stop, run_manager, verbose=self.verbose, **kwargs\n",
      "    )\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langchain_ollama/chat_models.py\", line 759, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "                 ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langchain_ollama/chat_models.py\", line 846, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/langchain_ollama/chat_models.py\", line 745, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"qwen2.5:1.5b\" not found, try pulling it first (status code: 404)\n",
      "During task with name 'generate' and id '1cf1d160-ee1c-2ef9-c2a5-634fd364dd4e'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 인코딩 강제 설정 (Jupyter 노트북 호환)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter 환경에서는 reconfigure 대신 환경변수로 처리\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter 노트북이나 다른 환경에서는 패스\n",
    "    pass\n",
    "\n",
    "# 모델 초기화 - 한글 처리 개선\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    model=\"qwen2.5:1.5b\", \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# LangGraph State 정의\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "# 개선된 프롬프트 템플릿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"당신은 한국어로 응답하는 AI 어시스턴트입니다. \n",
    "        반드시 한국어로만 답변하세요.\n",
    "        \n",
    "        당신의 작업:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 한국어 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "        \n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "        \n",
    "        중요: 반드시 한국어로만 응답하세요.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"질문: {question}\n",
    "        \n",
    "        추론 과정: {thinking}\n",
    "        \n",
    "        위 내용을 바탕으로 한국어로 답변해주세요:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"문자열이 UTF-8로 제대로 인코딩되었는지 확인하고 변환\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        return text.decode('utf-8', errors='ignore')\n",
    "    return str(text)\n",
    "\n",
    "# DeepSeek를 통해서 추론 부분까지만 생성\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] 입력 질문: {question}\")\n",
    "    print(f\"[DEBUG] 질문 타입: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    thinking_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 추론 결과 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 길이: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 미리보기: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5를 통해서 결과 출력 부분을 생성\n",
    "def generate(state: State):\n",
    "    question = ensure_utf8_string(state[\"question\"])\n",
    "    thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    print(f\"[DEBUG] generate 함수 - 질문: {question}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 길이: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 미리보기: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] 프롬프트 메시지 생성 완료\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    answer_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 최종 응답 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 길이: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 내용: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# 그래프 구성 및 컴파일\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "def main():\n",
    "    # 입력 데이터\n",
    "    inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"LangGraph 실행 시작\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # invoke()를 사용하여 그래프 호출\n",
    "        result = graph.invoke(inputs)\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(\"실행 결과\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"전체 결과: {result}\")\n",
    "        print(f\"최종 답변: {result.get('answer', '답변 없음')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 입력 질문: 9.9와 9.11 중 무엇이 더 큰가요?\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\n",
      "\n",
      "Now, comparing each digit after the decimal point:\n",
      "\n",
      "- The tenths place: Both numbers have 9.\n",
      "- The hundredths place: 9 has 9 and 11 has 0.\n",
      "\n",
      "Since 9 is greater than 0 in the hundredths place for 9.90 compared to 9.11, it follows that 9.9 is larger than 9.11.\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 466\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as...\n",
      "[DEBUG] generate 함수 - 질문: 9.9와 9.11 중 무엇이 더 큰가요?\n",
      "[DEBUG] generate 함수 - 추론 길이: 466\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9.\n",
      "\n",
      "To make a fair comparison, I'll align their decimal places by writing 9.9 as...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "<think>\n",
      "Okay, let's see. The user is asking whether 9.9 is larger than 9.11. The explanation provided breaks it down by comparing the whole numbers and then the decimal parts.\n",
      "\n",
      "First, both numbers start with 9, so the whole number parts are equal. Then, they look at the decimal parts. To compare, they convert 9.9 to 9.90 to align the decimal places. Now, comparing the tenths place: both are 9. Then, the hundredths place: 9.90 has a 0 in the hundredths, while 9.11 has a 1. Since 0 is greater than 1? Wait, no, 0 is less than 1. So 9.90 is actually smaller than 9.11. Wait, that's the opposite of what the initial thought said. Hmm, maybe there's a mistake here.\n",
      "\n",
      "Wait, the user's explanation says that 9.9 is larger because 9 has a 9 in the hundredths place and 9.11 has 0. But that's not correct. If you have 9.90 vs 9.11, the hundredths digit in 9.90 is 0, and in 9.11 it's 1. So 9.11 is larger because 1 is greater than 0. So the initial explanation might have an error here. The user's answer might have a mistake. But the user's instruction says to use the provided reasoning. So maybe the assistant is supposed to follow the given reasoning even if there's an error. But in reality, the correct answer is 9.11 is larger. So the assistant should correct that in the answer.\n",
      "\n",
      "But the user's reasoning says that since 9.90 has 9 in the hundredths place and 9.1[DEBUG] 입력 질문: 파이썬이란 무엇인가요\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "1 has 0, 9.9 is larger. But that's wrong. The correct comparison would be that 9.90 is 9.90, and 9.11 is 9.11. So 9.90 is larger than 9.11. Therefore, the correct answer is 9.9 is larger. Wait, but the user's reasoning says that 9.9 is larger because the hundredths place in 9.90 is 9 (since 9.9 is 9.90) and 9.11 has 0. Wait, maybe the user made a mistake in the reasoning. But the assistant has to follow the given reasoning. So the assistant's answer should reflect the given reasoning, even if it's incorrect. But the user's answer might have an error. However, the user's instruction says to use the provided reasoning. So the assistant should present the reasoning as given, even if it's incorrect. But the correct answer is 9.9 is larger. Wait, but the reasoning provided by the user says that 9.9 is larger because 9.90 has 9 in the hundredths place and 9.11 has 0. So the user's reasoning is correct. Wait, no. If 9.9 is 9.90, then comparing to 9.11, the hundredths place of 9.90 is 0, not 9. So the user's reasoning is wrong. Therefore, the assistant should correct that in the answer.\n",
      "\n",
      "But the user's instruction says to use the provided reasoning. So the assistant must present the reasoning as given, even if it's incorrect. But the correct answer is 9.11 is larger. However, the assistant's answer should be based on the given reasoning. Wait, but the assistant is supposed to generate a response based on the provided reasoning. So even if the reasoning is incorrect, the assistant must present it. But in reality, the correct answer is 9.9 is larger. So the assistant's answer should state that, but based on the given reasoning, which might be incorrect. However, the assistant must follow the user's provided reasoning. So the assistant's answer would be that 9.9 is larger because the hundredths place in 9.90 is 9 (since 9.9 is 9.90) and 9.11 has 0. But that's incorrect. So the assistant should note that there's an error in the reasoning but still present the answer as per the given reasoning. Alternatively, the assistant might have to correct it. But the user's instruction says to use the provided reasoning, so the assistant must present the reasoning as given, even if it's incorrect. Therefore, the assistant's answer would state that 9.9 is larger, based on the given reasoning. But in reality, the correct answer is 9.11 is larger. However, the assistant is supposed to follow the given reasoning. So the assistant's answer would be as per the given reasoning, even if it's wrong. Therefore, the assistant's answer would be: \"9.9는 9.11보다 더 큰 수입니다. 9.9는 9.90과 비교했을 때, 9.11은 9.11과 같은 수로, 9.90이 더 큰 수입니다. 따라서 9.9은 9.1[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 4409\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "Okay, so I'm trying to understand what Python is. From what I know, it's a programming language, but I'm not entirely sure how it all fits together. Let me start by breaking down the name \"Pyt...\n",
      "[DEBUG] generate 함수 - 질문: 파이썬이란 무엇인가요\n",
      "[DEBUG] generate 함수 - 추론 길이: 4409\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "Okay, so I'm trying to understand what Python is. From what I know, it's a programming language, but I'm not entirely sure how it all fits together. Let me start by breaking down the name \"Pyt...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "1보다 더 큰 수입니다.\"\n",
      "\n",
      "Wait, but the user's reasoning says that 9.9 is larger because 9.90 has 9 in the hundredths place and 9.11 has 0. But that's incorrect. Because 9.90's hundredths place is 0, not 9. So the assistant's answer should correct that. But the user's instruction says to use the provided reasoning. So the assistant must proceed as per the given reasoning, even if it's wrong. Therefore, the assistant's answer would be: \"9.9는 9.11보다 더 큰 수입니다. 9.9는 9.90과 비교했을 때, 9.11은 9.11과 같은 수로, 9.90이 더 큰 수입니다. 따라서 9.9은 9.11보다 더 큰 수입니다.\"\n",
      "\n",
      "But in reality, the correct answer is 9.11 is larger. However, the assistant must follow the provided reasoning. So the assistant's answer is as above.\n",
      "</think>\n",
      "\n",
      "9.9는 9.11보다 더 큰 수입니다. 9.9는 9.90으로 바꾸면, 9.90과 9.11을 비교할 때, 9.90의 9.90은 9.11의 9.11보다 0.8의 차이가 있습니다. 따라서 9.9는 9.11보다 더 큰 수입니다.[DEBUG] 최종 응답 타입: <class 'str'>\n",
      "[DEBUG] 최종 응답 길이: 4831\n",
      "[DEBUG] 최종 응답 내용: <think>\n",
      "Okay, let's see. The user is asking whether 9.9 is larger than 9.11. The explanation provided breaks it down by comparing the whole numbers and then the decimal parts.\n",
      "\n",
      "First, both numbers start with 9, so the whole number parts are equal. Then, they look at the decimal parts. To compare, they convert 9.9 to 9.90 to align the decimal places. Now, comparing the tenths place: both are 9. Then, the hundredths place: 9.90 has a 0 in the hundredths, while 9.11 has a 1. Since 0 is greater than 1? Wait, no, 0 is less than 1. So 9.90 is actually smaller than 9.11. Wait, that's the opposite of what the initial thought said. Hmm, maybe there's a mistake here.\n",
      "\n",
      "Wait, the user's explanation says that 9.9 is larger because 9 has a 9 in the hundredths place and 9.11 has 0. But that's not correct. If you have 9.90 vs 9.11, the hundredths digit in 9.90 is 0, and in 9.11 it's 1. So 9.11 is larger because 1 is greater than 0. So the initial explanation might have an error here. The user's answer might have a mistake. But the user's instruction says to use the provided reasoning. So maybe the assistant is supposed to follow the given reasoning even if there's an error. But in reality, the correct answer is 9.11 is larger. So the assistant should correct that in the answer.\n",
      "\n",
      "But the user's reasoning says that since 9.90 has 9 in the hundredths place and 9.11 has 0, 9.9 is larger. But that's wrong. The correct comparison would be that 9.90 is 9.90, and 9.11 is 9.11. So 9.90 is larger than 9.11. Therefore, the correct answer is 9.9 is larger. Wait, but the user's reasoning says that 9.9 is larger because the hundredths place in 9.90 is 9 (since 9.9 is 9.90) and 9.11 has 0. Wait, maybe the user made a mistake in the reasoning. But the assistant has to follow the given reasoning. So the assistant's answer should reflect the given reasoning, even if it's incorrect. But the user's answer might have an error. However, the user's instruction says to use the provided reasoning. So the assistant should present the reasoning as given, even if it's incorrect. But the correct answer is 9.9 is larger. Wait, but the reasoning provided by the user says that 9.9 is larger because 9.90 has 9 in the hundredths place and 9.11 has 0. So the user's reasoning is correct. Wait, no. If 9.9 is 9.90, then comparing to 9.11, the hundredths place of 9.90 is 0, not 9. So the user's reasoning is wrong. Therefore, the assistant should correct that in the answer.\n",
      "\n",
      "But the user's instruction says to use the provided reasoning. So the assistant must present the reasoning as given, even if it's incorrect. But the correct answer is 9.11 is larger. However, the assistant's answer should be based on the given reasoning. Wait, but the assistant is supposed to generate a response based on the provided reasoning. So even if the reasoning is incorrect, the assistant must present it. But in reality, the correct answer is 9.9 is larger. So the assistant's answer should state that, but based on the given reasoning, which might be incorrect. However, the assistant must follow the user's provided reasoning. So the assistant's answer would be that 9.9 is larger because the hundredths place in 9.90 is 9 (since 9.9 is 9.90) and 9.11 has 0. But that's incorrect. So the assistant should note that there's an error in the reasoning but still present the answer as per the given reasoning. Alternatively, the assistant might have to correct it. But the user's instruction says to use the provided reasoning, so the assistant must present the reasoning as given, even if it's incorrect. Therefore, the assistant's answer would state that 9.9 is larger, based on the given reasoning. But in reality, the correct answer is 9.11 is larger. However, the assistant is supposed to follow the given reasoning. So the assistant's answer would be as per the given reasoning, even if it's wrong. Therefore, the assistant's answer would be: \"9.9는 9.11보다 더 큰 수입니다. 9.9는 9.90과 비교했을 때, 9.11은 9.11과 같은 수로, 9.90이 더 큰 수입니다. 따라서 9.9은 9.11보다 더 큰 수입니다.\"\n",
      "\n",
      "Wait, but the user's reasoning says that 9.9 is larger because 9.90 has 9 in the hundredths place and 9.11 has 0. But that's incorrect. Because 9.90's hundredths place is 0, not 9. So the assistant's answer should correct that. But the user's instruction says to use the provided reasoning. So the assistant must proceed as per the given reasoning, even if it's wrong. Therefore, the assistant's answer would be: \"9.9는 9.11보다 더 큰 수입니다. 9.9는 9.90과 비교했을 때, 9.11은 9.11과 같은 수로, 9.90이 더 큰 수입니다. 따라서 9.9은 9.11보다 더 큰 수입니다.\"\n",
      "\n",
      "But in reality, the correct answer is 9.11 is larger. However, the assistant must follow the provided reasoning. So the assistant's answer is as above.\n",
      "</think>\n",
      "\n",
      "9.9는 9.11보다 더 큰 수입니다. 9.9는 9.90으로 바꾸면, 9.90과 9.11을 비교할 때, 9.90의 9.90은 9.11의 9.11보다 0.8의 차이가 있습니다. 따라서 9.9는 9.11보다 더 큰 수입니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 최종 응답 타입: <class 'str'>\n",
      "[DEBUG] 최종 응답 길이: 745\n",
      "[DEBUG] 최종 응답 내용: <think>\n",
      "</think>\n",
      "\n",
      "파이썬은 프로그래밍 언어 중 하나로, 코드를 작성하여 컴퓨터가 수행할 수 있는 명령어를 정의하는 데 사용됩니다. 그 이름은 \"Python\"으로, \"py\"는 \"programmer\" 또는 \"pythonic\"과 관련되어 있고, \"lacuna\"는 코드의 결함이나 구조적 특징을 의미합니다. 파이썬은 간결하고 직관적인 문법을 가진 언어로, 데이터 과학, 머신러닝, 웹 개발 등 다양한 분야에서 널리 사용됩니다.\n",
      "\n",
      "파이썬은 인터프리터 언어이므로, 코드를 작성하고 실행할 수 있는 즉시 실행 가능한 방식으로, 컴파일 단계를 거치지 않고 실행할 수 있습니다. 이는 편리하게 작업을 수행할 수 있게 해줍니다. 기본적인 구조는 `if-else`, `for`, `while` 등의 조건문과 반복문, 그리고 함수, 클래스 등을 포함하며, 코드의 간결함과 편의성을 특징으로 합니다.\n",
      "\n",
      "파이썬은 오픈소스로 제공되며, 다양한 라이브러리와 커뮤니티가 존재하여 다양한 기능을 수행할 수 있습니다. 예를 들어, 데이터 분석에는 Pandas, 머신러닝에는 Scikit-learn, 웹 개발에는 Django, Flask 등이 있습니다. 이처럼 파이썬은 문제에 따라 다양한 용도로 활용할 수 있는 유연한 언어입니다.\n",
      "\n",
      "이러한 특성으로 인해 파이썬은 초보자부터 경계 없는 프로그래밍을 시작하는 사람들에게 좋은 언어로 알려져 있습니다. 기본 문법을 익히고, 실습을 통해 다양한 프로젝트를 진행해 나가는 과정이 학습의 핵심이 될 수 있습니다.\n",
      "[DEBUG] 입력 질문: 파이썬이란?\n",
      "[DEBUG] 질문 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 타입: <class 'str'>\n",
      "[DEBUG] 추론 결과 길이: 9\n",
      "[DEBUG] 추론 결과 미리보기: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] generate 함수 - 질문: 파이썬이란?\n",
      "[DEBUG] generate 함수 - 추론 길이: 9\n",
      "[DEBUG] generate 함수 - 추론 미리보기: <think>\n",
      "\n",
      "...\n",
      "[DEBUG] 프롬프트 메시지 생성 완료\n",
      "[DEBUG] 최종 응답 타입: <class 'str'>\n",
      "[DEBUG] 최종 응답 길이: 1590\n",
      "[DEBUG] 최종 응답 내용: <think>\n",
      "Okay, the user is asking what Python is. Let me start by recalling the basic definition. Python is a programming language, right? It's known for being easy to learn and powerful for various tasks. I should mention that it's widely used in web development, data analysis, scientific computing, and more.\n",
      "\n",
      "Wait, the user provided an inference process, but I need to make sure I don't mention it. The key points are that Python is a high-level, general-purpose language, created by Guido van Rossum in 1991. It has a simple syntax, which makes it accessible for beginners. Also, it's open-source and has a vast ecosystem with libraries and frameworks.\n",
      "\n",
      "I should structure the answer to highlight its main features: simplicity, versatility, community support, and the availability of libraries. Maybe mention some popular uses like Django for web apps, NumPy for data analysis, and TensorFlow for machine learning. Also, note that it's cross-platform, meaning it works on different operating systems.\n",
      "\n",
      "Avoid technical jargon but keep it informative. Make sure the answer is concise but covers the essentials. Check if there's anything else the user might need, but since the question is straightforward, focus on the core aspects of Python's characteristics and applications.\n",
      "</think>\n",
      "\n",
      "파이썬은 고급 단계의 다목적 프로그래밍 언어로, 간결하고 명확한 문법을 특징으로 합니다. 이 언어는 웹 개발, 데이터 분석, 사이언티픽 컴퓨팅, 머신러닝 등 다양한 분야에서 널리 활용되며, 빠르게 성장하고 있는 개방형 플랫폼입니다. 파이썬은 코드를 작성할 때 자연스럽게 익숙해지는 쉬운 구문과 대규모 커뮤니티의 지원이 특징입니다. 이를 통해 신입 개발자도 쉽게 학습할 수 있어 대학 및 학교에서 널리 사용되고 있으며, 주요 라이브러리와 프레임워크(예: Django, NumPy, TensorFlow)가 존재하여 다양한 기능을 제공합니다.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rose/Library/Caches/pypoetry/virtualenvs/mylangchain-app-75wTJiYg-py3.13/lib/python3.13/site-packages/gradio/chat_interface.py:348: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 인코딩 강제 설정 (Jupyter 노트북 호환)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter 환경에서는 reconfigure 대신 환경변수로 처리\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter 노트북이나 다른 환경에서는 패스\n",
    "    pass\n",
    "\n",
    "# 모델 설정: 두 개의 서로 다른 모델을 사용하여 추론과 답변 생성을 수행\n",
    "# - reasoning_model: 추론을 담당하는 모델 (온도 낮음, 정확한 분석용)\n",
    "# - generation_model: 답변 생성을 담당하는 모델 (온도 높음, 창의적 응답용)\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    model=\"qwen3:1.7b\", \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 상태(State) 정의: 그래프에서 상태를 유지하기 위한 데이터 구조\n",
    "class State(TypedDict):\n",
    "    question: str   # 사용자의 질문\n",
    "    thinking: str   # 추론 결과\n",
    "    answer: str     # 최종 답변\n",
    "\n",
    "# 개선된 프롬프트 템플릿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"당신은 한국어로 응답하는 AI 어시스턴트입니다. \n",
    "        반드시 한국어로만 답변하세요.\n",
    "        \n",
    "        당신의 작업:\n",
    "        - 질문에 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 한국어 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "        \n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "        \n",
    "        중요: 반드시 한국어로만 응답하세요.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"질문: {question}\n",
    "        \n",
    "        추론 과정: {thinking}\n",
    "        \n",
    "        위 내용을 바탕으로 한국어로 답변해주세요:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"문자열이 UTF-8로 제대로 인코딩되었는지 확인하고 변환\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            return text.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # 문자열이지만 인코딩 문제가 있을 수 있는 경우 처리\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # 문자열을 UTF-8로 인코딩했다가 다시 디코딩하여 정리\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            return text\n",
    "    \n",
    "    return str(text)\n",
    "\n",
    "# DeepSeek를 통해서 추론 부분까지만 생성\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] 입력 질문: {question}\")\n",
    "    print(f\"[DEBUG] 질문 타입: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    thinking_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 추론 결과 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 길이: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] 추론 결과 미리보기: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5를 통해서 결과 출력 부분을 생성\n",
    "def generate(state: State):\n",
    "    question = ensure_utf8_string(state[\"question\"])\n",
    "    thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    print(f\"[DEBUG] generate 함수 - 질문: {question}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 길이: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate 함수 - 추론 미리보기: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] 프롬프트 메시지 생성 완료\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    answer_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] 최종 응답 타입: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 길이: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] 최종 응답 내용: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# 그래프 생성 함수: 상태(State) 간의 흐름을 정의\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio 인터페이스 생성 및 실행\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "#iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "def launch_gradio():\n",
    "    #iface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "    iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI 챗봇\", description=\"질문을 입력하면 AI가 답변을 제공합니다.\")\n",
    "    iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #iface.launch()\n",
    "    launch_gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-75wTJiYg-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
