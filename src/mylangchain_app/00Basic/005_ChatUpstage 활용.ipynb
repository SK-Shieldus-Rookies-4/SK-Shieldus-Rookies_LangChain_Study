{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add pypdf=\">=4.2.0,<5.0.0\"\n",
    "# poetry add langchain-upstage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "- `.env` íŒŒì¼ì— `UPSTAGE_API_KEY` ë“±ë¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up_z\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LLM ë‹µë³€ ìƒì„±\n",
    "\n",
    "- Upstage Consoleì—ì„œ ë°œê¸‰ë°›ì€ API Keyë¥¼ `UPSTAGE_API_KEY`ë¼ê³  ì €ì¥í•˜ë©´ ë³„ë„ì˜ ì„¤ì • ì—†ì´ `ChatUpstage`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x135988e10> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x135989810> model_name='solar-pro' temperature=0.5 model_kwargs={} upstage_api_key=SecretStr('**********') upstage_api_base='https://api.upstage.ai/v1'\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "#llm = ChatUpstage(temperature=0.5)\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "**LangChain**ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM, Large Language Model)ì„ í™œìš©í•´ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ **ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬**ì…ë‹ˆë‹¤. ë³µì¡í•œ ì‘ì—…ì„ ì²˜ë¦¬í•  ë•Œ LLMì˜ ëŠ¥ë ¥ì„ í™•ì¥í•˜ê±°ë‚˜ ë³´ì™„í•˜ëŠ” ë„êµ¬, ëª¨ë“ˆ, íŒ¨í„´ì„ ì œê³µí•©ë‹ˆë‹¤.  \n",
      "\n",
      "### ğŸ“Œ **í•µì‹¬ ê°œë…**  \n",
      "1. **ëª¨ë“ˆì‹ ì„¤ê³„**  \n",
      "   - LLMì˜ ê¸°ëŠ¥ì„ ë³´ì™„í•˜ëŠ” ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸(ë©”ëª¨ë¦¬, ê²€ìƒ‰, ì²´ì¸, ì—ì´ì „íŠ¸ ë“±)ë¥¼ ì¡°í•©í•´ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "   - ì˜ˆ: ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ â†’ LLMì— ì»¨í…ìŠ¤íŠ¸ ì œê³µ â†’ ì‚¬ìš©ì ì‘ë‹µ ìƒì„±.  \n",
      "\n",
      "2. **ì£¼ìš” êµ¬ì„± ìš”ì†Œ**  \n",
      "   - **ëª¨ë¸ I/O**: LLM(ì˜ˆ: GPT, Claude)ê³¼ì˜ ì¸í„°í˜ì´ìŠ¤.  \n",
      "   - **ì²´ì¸(Chains)**: ì—¬ëŸ¬ ë‹¨ê³„(ì˜ˆ: ê²€ìƒ‰ â†’ ìš”ì•½ â†’ ë‹µë³€)ë¥¼ ì—°ê²°í•˜ëŠ” íŒŒì´í”„ë¼ì¸.  \n",
      "   - **ì—ì´ì „íŠ¸(Agents)**: ë™ì ìœ¼ë¡œ ë„êµ¬ë¥¼ ì„ íƒí•´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ììœ¨ ì‹œìŠ¤í…œ.  \n",
      "   - **ë©”ëª¨ë¦¬(Memory)**: ëŒ€í™” ê¸°ë¡ì´ë‚˜ ìƒíƒœ ìœ ì§€ë¥¼ ì§€ì›.  \n",
      "   - **ë¬¸ì„œ ë¡œë”/ë¶„í• ê¸°**: PDF, ì›¹ í˜ì´ì§€ ë“± ì™¸ë¶€ ë°ì´í„° ì²˜ë¦¬.  \n",
      "\n",
      "3. **ì‚¬ìš© ì‚¬ë¡€**  \n",
      "   - ì±—ë´‡, ë¬¸ì„œ Q&A ì‹œìŠ¤í…œ, ì½”ë“œ ìƒì„± ë„êµ¬, ê°œì¸í™”ëœ ì½˜í…ì¸  ìƒì„± ë“±.  \n",
      "   - ì˜ˆ: ì‚¬ìš©ì ì§ˆë¬¸ â†’ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ â†’ LLMì´ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±.  \n",
      "\n",
      "4. **í†µí•© ì§€ì›**  \n",
      "   - Hugging Face, OpenAI, Anthropic ë“± ë‹¤ì–‘í•œ LLM ê³µê¸‰ìì™€ í˜¸í™˜.  \n",
      "   - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤(FAISS, Pinecone)ì™€ ì—°ë™í•´ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ê°€ëŠ¥.  \n",
      "\n",
      "### ğŸ› ï¸ **ê°„ë‹¨í•œ ì˜ˆì‹œ**  \n",
      "```python\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "# 1. ë¬¸ì„œ ì„ë² ë”© ë° ì €ì¥\n",
      "embedding = OpenAIEmbeddings()\n",
      "vectorstore = Chroma.from_documents(texts, embedding)\n",
      "\n",
      "# 2. ê²€ìƒ‰ ê¸°ë°˜ QA ì²´ì¸ êµ¬ì„±\n",
      "qa_chain = RetrievalQA.from_chain_type(\n",
      "    llm=OpenAI(),\n",
      "    chain_type=\"stuff\",\n",
      "    retriever=vectorstore.as_retriever()\n",
      ")\n",
      "\n",
      "# 3. ì§ˆë¬¸ ì²˜ë¦¬\n",
      "result = qa_chain.run(\"ë‚´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
      "print(result)\n",
      "```\n",
      "\n",
      "### ğŸ” **ì™œ ì‚¬ìš©í• ê¹Œ?**  \n",
      "- LLMë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•œ **ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬**, **ë„êµ¬ í™œìš©**, **ì‘ì—… íë¦„ ì œì–´**ë¥¼ í•´ê²°í•©ë‹ˆë‹¤.  \n",
      "- ë³µì¡í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ **ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ**ë¡œ ë¹ ë¥´ê²Œ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
      "\n",
      "LangChainì€ [ê³µì‹ ë¬¸ì„œ](https://python.langchain.com/)ì—ì„œ ë” ìì„¸íˆ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "ai_message=llm.invoke(\"LangChainì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "print(type(ai_message))\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangChain**ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ê°„ì†Œí™”í•˜ê¸° ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. 2022ë…„ Harrison Chaseê°€ ì£¼ë„í•˜ì—¬ ê°œë°œë˜ì—ˆìœ¼ë©°, LLMì˜ ì ì¬ë ¥ì„ ìµœëŒ€í•œ ëŒì–´ë‚´ê¸° ìœ„í•´ ì™¸ë¶€ ë„êµ¬, ë©”ëª¨ë¦¬, ë°ì´í„° ì†ŒìŠ¤ì™€ì˜ í†µí•©ì„ ìš©ì´í•˜ê²Œ í•˜ëŠ” ê²ƒì´ í•µì‹¬ ëª©í‘œì…ë‹ˆë‹¤.\n",
      "\n",
      "### ğŸ“Œ **ì£¼ìš” ê°œë… ë° íŠ¹ì§•**\n",
      "1. **ëª¨ë“ˆì‹ ì„¤ê³„**  \n",
      "   - **Components**: LLM í˜¸ì¶œ, í”„ë¡¬í”„íŠ¸ ê´€ë¦¬, ì²´ì¸(Chain), ì—ì´ì „íŠ¸(Agent), ë©”ëª¨ë¦¬(Memory) ë“± ëª¨ë“ˆë¡œ êµ¬ì„±ë˜ì–´ ìœ ì—°í•˜ê²Œ ì¡°í•© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "   - **Chain**: ì—¬ëŸ¬ LLM í˜¸ì¶œê³¼ ì™¸ë¶€ ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” íŒŒì´í”„ë¼ì¸ (ì˜ˆ: ì§ˆë¬¸ â†’ ê²€ìƒ‰ â†’ ë‹µë³€ ìƒì„±).\n",
      "\n",
      "2. **ì£¼ìš” ê¸°ëŠ¥**\n",
      "   - **í”„ë¡¬í”„íŠ¸ ê´€ë¦¬**: ë™ì  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿(`PromptTemplate`) ì§€ì›.\n",
      "   - **ë©”ëª¨ë¦¬ í†µí•©**: ëŒ€í™” ê¸°ë¡, ìƒíƒœ ì €ì¥ ê¸°ëŠ¥ (ì±„íŒ…ë´‡ì— ìœ ìš©).\n",
      "   - **ì—ì´ì „íŠ¸**: LLMì´ ììœ¨ì ìœ¼ë¡œ ë„êµ¬(ê³„ì‚°ê¸°, ê²€ìƒ‰ ì—”ì§„ ë“±)ë¥¼ ì„ íƒí•´ ì‘ì—… ìˆ˜í–‰ (ReAct íŒ¨í„´ ê¸°ë°˜).\n",
      "   - **ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°**: SQL, ë²¡í„° DB(FAISS, Pinecone), API ë“±ê³¼ì˜ ì—°ë™.\n",
      "\n",
      "3. **ì§€ì› LLM**  \n",
      "   OpenAI, Anthropic, Meta, ë¡œì»¬ ëª¨ë¸(Llama.cpp) ë“± ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ í˜¸í™˜ë©ë‹ˆë‹¤.\n",
      "\n",
      "4. **ì‚¬ìš© ì‚¬ë¡€**  \n",
      "   - ì±—ë´‡, ë¬¸ì„œ ìš”ì•½, ì½”ë“œ ìƒì„±, ë°ì´í„° ë¶„ì„ ìë™í™” ë“±.\n",
      "\n",
      "### ğŸ›  **ì˜ˆì‹œ ì½”ë“œ (ê°„ë‹¨í•œ ì²´ì¸)**\n",
      "```python\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
      "prompt = PromptTemplate(input_variables=[\"topic\"], template=\"ë‹¤ìŒ ì£¼ì œë¡œ 3ë¬¸ì¥ ìš”ì•½: {topic}\")\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "# ì‹¤í–‰\n",
      "chain.run(\"LangChainì˜ í•µì‹¬ ê¸°ëŠ¥\")\n",
      "```\n",
      "\n",
      "### ğŸ“ˆ **ì¥ì **\n",
      "- **í™•ì¥ì„±**: ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì¶”ê°€ ê°€ëŠ¥.\n",
      "- **ìƒì‚°ì„±**: ë°˜ë³µ ì‘ì—…ì„ ì²´ì¸ìœ¼ë¡œ ì¶”ìƒí™”.\n",
      "- **ì»¤ë®¤ë‹ˆí‹°**: í™œë°œí•œ ìƒíƒœê³„(í—ˆê¹…í˜ì´ìŠ¤, LangSmith í†µí•©).\n",
      "\n",
      "### ğŸ” **ê´€ë ¨ ë„êµ¬**\n",
      "- **LangSmith**: ë””ë²„ê¹… ë° ìµœì í™”ë¥¼ ìœ„í•œ ê´€ì¸¡ ê°€ëŠ¥ì„± ë„êµ¬.\n",
      "- **LlamaIndex**: ë¬¸ì„œ ê¸°ë°˜ RAG(Retrieval-Augmented Generation)ì™€ ì—°ë™.\n",
      "\n",
      "LangChainì€ LLMì„ \"ë ˆê³  ë¸”ë¡\"ì²˜ëŸ¼ ì¡°ë¦½í•´ ë³µì¡í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í˜„ëŒ€ì ì¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ğŸš€"
     ]
    }
   ],
   "source": [
    "# using chat stream\n",
    "for chunk in llm.stream(\"LangChainì€ ë¬´ì—‡ì¸ê°€ìš”?\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upstage Response:\n",
      "\"LangChainì€ AI ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•ì„ ìœ„í•œ ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\"  \n",
      "\n",
      "### ì¶”ê°€ ì„¤ëª… (í•„ìš”ì‹œ ì°¸ê³ ):  \n",
      "- \"LangChain\"ì€ ê¸°ìˆ  ìš©ì–´ë¡œ ê·¸ëŒ€ë¡œ ìŒì°¨í•˜ì—¬ í‘œê¸°í–ˆìŠµë‹ˆë‹¤.  \n",
      "- \"framework\"ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ë§¥ë½ì—ì„œ í”íˆ \"í”„ë ˆì„ì›Œí¬\"ë¡œ ë²ˆì—­ë˜ë©°, \"ê¸°ë°˜ êµ¬ì¡°\"ë‚˜ \"í”Œë«í¼\"ìœ¼ë¡œ ì˜ì—­í•  ìˆ˜ë„ ìˆì§€ë§Œ ì›ë¬¸ì˜ ë‰˜ì•™ìŠ¤ë¥¼ ë³´ì¡´í•˜ê¸° ìœ„í•´ ì§ì—­í–ˆìŠµë‹ˆë‹¤.  \n",
      "- \"powerful\"ì€ \"ê°•ë ¥í•œ\"ìœ¼ë¡œ ë²ˆì—­í•´ ê¸°ëŠ¥ê³¼ ì„±ëŠ¥ì„ ê°•ì¡°í•˜ëŠ” ì˜ë¯¸ë¥¼ ë‹´ì•˜ìŠµë‹ˆë‹¤.  \n",
      "\n",
      "ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:  \n",
      "- \"LangChainì€ ìƒì„±í˜• AI ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í•µì‹¬ ë„êµ¬ì…ë‹ˆë‹¤.\" (ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„)  \n",
      "- \"LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•ì— ìµœì í™”ëœ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\" (ì„¸ë¶€ ê¸°ëŠ¥ ê°•ì¡°)\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "translation_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a professional translator specializing in Korean-English translation.\"),\n",
    "        (\"human\", \"Translate this from {source_lang} to {target_lang}: {text}\")\n",
    "    ])\n",
    "\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "chain = translation_prompt | llm\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"source_lang\": \"English\",\n",
    "    \"target_lang\": \"Korean\", \n",
    "    \"text\": \"LangChain is a powerful framework for building AI applications.\"\n",
    "})\n",
    "\n",
    "print(\"Upstage Response:\")\n",
    "print(response.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ë¬¸ì¥ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì¤˜. Hello, How are you?.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "# using chain\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that translates English to Korean.\"),\n",
    "        (\"human\", \"Translate this sentence from English to Korean. {english_text}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatUpstage()\n",
    "chain = prompt | llm\n",
    "\n",
    "ai_message=chain.invoke({\"english_text\": \"Hello, How are you?\"})\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundness Check\n",
    "- Groundedness Check APIëŠ” ì‚¬ìš©ìê°€ ì œê³µí•œ Context(ì»¨í…ìŠ¤íŠ¸)ì— ëŒ€í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µì´ ì‹¤ì œë¡œ ê·¸ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ê³  ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grounded\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import UpstageGroundednessCheck\n",
    "\n",
    "groundedness_check = UpstageGroundednessCheck()\n",
    "\n",
    "request_input = {\n",
    "    \"context\": \"ì‚¼ì„±ì „ìëŠ” ì—°ê²° ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¶œ 74.07ì¡°ì›, ì˜ì—…ì´ìµ 10.44ì¡°ì›ì˜ 2024ë…„ 2ë¶„ê¸° ì‹¤ì ì„ ë°œí‘œí–ˆë‹¤. ì „ì‚¬ ë§¤ì¶œì€ ì „ë¶„ê¸° ëŒ€ë¹„ 3% ì¦ê°€í•œ 74.07ì¡°ì›ì„ ê¸°ë¡í–ˆë‹¤. DSë¶€ë¬¸ì€ ë©”ëª¨ë¦¬ ì—…í™© íšŒë³µìœ¼ë¡œ ì „ë¶„ê¸° ëŒ€ë¹„ 23% ì¦ê°€í•˜ê³ , SDCëŠ” OLED íŒë§¤ í˜¸ì¡°ë¡œ ì¦ê°€í–ˆë‹¤.\",\n",
    "    \"answer\": \"ì‚¼ì„±ì „ìì˜ 2024ë…„ 2ë¶„ê¸° ë§¤ì¶œì€ ì•½ 74.07ì¡°ì›ì´ë‹¤.\",\n",
    "}\n",
    "\n",
    "response = groundedness_check.invoke(request_input)\n",
    "print(response)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-75wTJiYg-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
