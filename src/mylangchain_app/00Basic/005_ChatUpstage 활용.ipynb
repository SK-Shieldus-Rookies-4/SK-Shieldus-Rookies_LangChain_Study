{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add pypdf=\">=4.2.0,<5.0.0\"\n",
    "# poetry add langchain-upstage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 환경변수 불러오기\n",
    "\n",
    "- `.env` 파일에 `UPSTAGE_API_KEY` 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up_z\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv()\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LLM 답변 생성\n",
    "\n",
    "- Upstage Console에서 발급받은 API Key를 `UPSTAGE_API_KEY`라고 저장하면 별도의 설정 없이 `ChatUpstage`를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x135988e10> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x135989810> model_name='solar-pro' temperature=0.5 model_kwargs={} upstage_api_key=SecretStr('**********') upstage_api_base='https://api.upstage.ai/v1'\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "#llm = ChatUpstage(temperature=0.5)\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "**LangChain**은 대규모 언어 모델(LLM, Large Language Model)을 활용해 애플리케이션을 구축하기 위한 **오픈소스 프레임워크**입니다. 복잡한 작업을 처리할 때 LLM의 능력을 확장하거나 보완하는 도구, 모듈, 패턴을 제공합니다.  \n",
      "\n",
      "### 📌 **핵심 개념**  \n",
      "1. **모듈식 설계**  \n",
      "   - LLM의 기능을 보완하는 다양한 컴포넌트(메모리, 검색, 체인, 에이전트 등)를 조합해 애플리케이션을 구성할 수 있습니다.  \n",
      "   - 예: 외부 데이터베이스 검색 → LLM에 컨텍스트 제공 → 사용자 응답 생성.  \n",
      "\n",
      "2. **주요 구성 요소**  \n",
      "   - **모델 I/O**: LLM(예: GPT, Claude)과의 인터페이스.  \n",
      "   - **체인(Chains)**: 여러 단계(예: 검색 → 요약 → 답변)를 연결하는 파이프라인.  \n",
      "   - **에이전트(Agents)**: 동적으로 도구를 선택해 작업을 수행하는 자율 시스템.  \n",
      "   - **메모리(Memory)**: 대화 기록이나 상태 유지를 지원.  \n",
      "   - **문서 로더/분할기**: PDF, 웹 페이지 등 외부 데이터 처리.  \n",
      "\n",
      "3. **사용 사례**  \n",
      "   - 챗봇, 문서 Q&A 시스템, 코드 생성 도구, 개인화된 콘텐츠 생성 등.  \n",
      "   - 예: 사용자 질문 → 관련 문서 검색 → LLM이 문서 기반으로 답변 생성.  \n",
      "\n",
      "4. **통합 지원**  \n",
      "   - Hugging Face, OpenAI, Anthropic 등 다양한 LLM 공급자와 호환.  \n",
      "   - 벡터 데이터베이스(FAISS, Pinecone)와 연동해 컨텍스트 관리 가능.  \n",
      "\n",
      "### 🛠️ **간단한 예시**  \n",
      "```python\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "# 1. 문서 임베딩 및 저장\n",
      "embedding = OpenAIEmbeddings()\n",
      "vectorstore = Chroma.from_documents(texts, embedding)\n",
      "\n",
      "# 2. 검색 기반 QA 체인 구성\n",
      "qa_chain = RetrievalQA.from_chain_type(\n",
      "    llm=OpenAI(),\n",
      "    chain_type=\"stuff\",\n",
      "    retriever=vectorstore.as_retriever()\n",
      ")\n",
      "\n",
      "# 3. 질문 처리\n",
      "result = qa_chain.run(\"내 문서의 주요 내용은 무엇인가요?\")\n",
      "print(result)\n",
      "```\n",
      "\n",
      "### 🔍 **왜 사용할까?**  \n",
      "- LLM만으로는 부족한 **컨텍스트 관리**, **도구 활용**, **작업 흐름 제어**를 해결합니다.  \n",
      "- 복잡한 애플리케이션을 **재사용 가능한 모듈**로 빠르게 개발할 수 있습니다.  \n",
      "\n",
      "LangChain은 [공식 문서](https://python.langchain.com/)에서 더 자세히 확인할 수 있습니다. 😊\n"
     ]
    }
   ],
   "source": [
    "ai_message=llm.invoke(\"LangChain은 무엇인가요?\")\n",
    "print(type(ai_message))\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangChain**은 대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 간소화하기 위한 오픈소스 프레임워크입니다. 2022년 Harrison Chase가 주도하여 개발되었으며, LLM의 잠재력을 최대한 끌어내기 위해 외부 도구, 메모리, 데이터 소스와의 통합을 용이하게 하는 것이 핵심 목표입니다.\n",
      "\n",
      "### 📌 **주요 개념 및 특징**\n",
      "1. **모듈식 설계**  \n",
      "   - **Components**: LLM 호출, 프롬프트 관리, 체인(Chain), 에이전트(Agent), 메모리(Memory) 등 모듈로 구성되어 유연하게 조합 가능합니다.\n",
      "   - **Chain**: 여러 LLM 호출과 외부 작업을 순차적으로 연결하는 파이프라인 (예: 질문 → 검색 → 답변 생성).\n",
      "\n",
      "2. **주요 기능**\n",
      "   - **프롬프트 관리**: 동적 프롬프트 템플릿(`PromptTemplate`) 지원.\n",
      "   - **메모리 통합**: 대화 기록, 상태 저장 기능 (채팅봇에 유용).\n",
      "   - **에이전트**: LLM이 자율적으로 도구(계산기, 검색 엔진 등)를 선택해 작업 수행 (ReAct 패턴 기반).\n",
      "   - **데이터 소스 연결**: SQL, 벡터 DB(FAISS, Pinecone), API 등과의 연동.\n",
      "\n",
      "3. **지원 LLM**  \n",
      "   OpenAI, Anthropic, Meta, 로컬 모델(Llama.cpp) 등 다양한 모델과 호환됩니다.\n",
      "\n",
      "4. **사용 사례**  \n",
      "   - 챗봇, 문서 요약, 코드 생성, 데이터 분석 자동화 등.\n",
      "\n",
      "### 🛠 **예시 코드 (간단한 체인)**\n",
      "```python\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "llm = OpenAI(model_name=\"gpt-3.5-turbo\")\n",
      "prompt = PromptTemplate(input_variables=[\"topic\"], template=\"다음 주제로 3문장 요약: {topic}\")\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "# 실행\n",
      "chain.run(\"LangChain의 핵심 기능\")\n",
      "```\n",
      "\n",
      "### 📈 **장점**\n",
      "- **확장성**: 커스텀 모듈 추가 가능.\n",
      "- **생산성**: 반복 작업을 체인으로 추상화.\n",
      "- **커뮤니티**: 활발한 생태계(허깅페이스, LangSmith 통합).\n",
      "\n",
      "### 🔍 **관련 도구**\n",
      "- **LangSmith**: 디버깅 및 최적화를 위한 관측 가능성 도구.\n",
      "- **LlamaIndex**: 문서 기반 RAG(Retrieval-Augmented Generation)와 연동.\n",
      "\n",
      "LangChain은 LLM을 \"레고 블록\"처럼 조립해 복잡한 애플리케이션을 구축할 수 있게 해주는 현대적인 프레임워크입니다. 🚀"
     ]
    }
   ],
   "source": [
    "# using chat stream\n",
    "for chunk in llm.stream(\"LangChain은 무엇인가요?\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upstage Response:\n",
      "\"LangChain은 AI 애플리케이션 구축을 위한 강력한 프레임워크입니다.\"  \n",
      "\n",
      "### 추가 설명 (필요시 참고):  \n",
      "- \"LangChain\"은 기술 용어로 그대로 음차하여 표기했습니다.  \n",
      "- \"framework\"는 소프트웨어 개발 맥락에서 흔히 \"프레임워크\"로 번역되며, \"기반 구조\"나 \"플랫폼\"으로 의역할 수도 있지만 원문의 뉘앙스를 보존하기 위해 직역했습니다.  \n",
      "- \"powerful\"은 \"강력한\"으로 번역해 기능과 성능을 강조하는 의미를 담았습니다.  \n",
      "\n",
      "문맥에 따라 다음과 같이 변형할 수도 있습니다:  \n",
      "- \"LangChain은 생성형 AI 기반 애플리케이션 개발을 위한 핵심 도구입니다.\" (더 자연스러운 표현)  \n",
      "- \"LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션 구축에 최적화된 프레임워크입니다.\" (세부 기능 강조)\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "translation_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a professional translator specializing in Korean-English translation.\"),\n",
    "        (\"human\", \"Translate this from {source_lang} to {target_lang}: {text}\")\n",
    "    ])\n",
    "\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "# 체인 실행\n",
    "chain = translation_prompt | llm\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"source_lang\": \"English\",\n",
    "    \"target_lang\": \"Korean\", \n",
    "    \"text\": \"LangChain is a powerful framework for building AI applications.\"\n",
    "})\n",
    "\n",
    "print(\"Upstage Response:\")\n",
    "print(response.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 문장을 한국어로 번역해줘. Hello, How are you?.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "# using chain\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that translates English to Korean.\"),\n",
    "        (\"human\", \"Translate this sentence from English to Korean. {english_text}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatUpstage()\n",
    "chain = prompt | llm\n",
    "\n",
    "ai_message=chain.invoke({\"english_text\": \"Hello, How are you?\"})\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groundness Check\n",
    "- Groundedness Check API는 사용자가 제공한 Context(컨텍스트)에 대한 AI 어시스턴트의 응답이 실제로 그 컨텍스트에 기반하고 있는지 여부를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grounded\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import UpstageGroundednessCheck\n",
    "\n",
    "groundedness_check = UpstageGroundednessCheck()\n",
    "\n",
    "request_input = {\n",
    "    \"context\": \"삼성전자는 연결 기준으로 매출 74.07조원, 영업이익 10.44조원의 2024년 2분기 실적을 발표했다. 전사 매출은 전분기 대비 3% 증가한 74.07조원을 기록했다. DS부문은 메모리 업황 회복으로 전분기 대비 23% 증가하고, SDC는 OLED 판매 호조로 증가했다.\",\n",
    "    \"answer\": \"삼성전자의 2024년 2분기 매출은 약 74.07조원이다.\",\n",
    "}\n",
    "\n",
    "response = groundedness_check.invoke(request_input)\n",
    "print(response)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-75wTJiYg-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
